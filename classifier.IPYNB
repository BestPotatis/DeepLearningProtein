{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'rfl' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n rfl ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mUntitled-1.ipynb Cell 2\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#W4sdW50aXRsZWQ%3D?line=0'>1</a>\u001b[0m \u001b[39m# Define the base directory where your protein folders are located\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#W4sdW50aXRsZWQ%3D?line=1'>2</a>\u001b[0m \u001b[39mprint\u001b[39m(os\u001b[39m.\u001b[39mgetcwd())\n\u001b[0;32m      <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#W4sdW50aXRsZWQ%3D?line=2'>3</a>\u001b[0m base_dir \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m/path/to/your/protein/folders\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#W4sdW50aXRsZWQ%3D?line=4'>5</a>\u001b[0m \u001b[39m# List to store loaded proteins\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "# Define the base directory where your protein folders are located\n",
    "print(os.getcwd())\n",
    "base_dir = '/path/to/your/protein/folders'\n",
    "\n",
    "# List to store loaded proteins\n",
    "all_proteins = []\n",
    "\n",
    "# Iterate through the folders 'cv0' to 'cv5'\n",
    "for i in range(6):  # Assuming folders are named 'cv0' to 'cv5'\n",
    "    folder_name = f'cv{i}'\n",
    "    folder_path = os.path.join(base_dir, folder_name)\n",
    "\n",
    "    # Iterate through files in the folder\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.npy'):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "\n",
    "            # Load the protein and append to the list\n",
    "            protein = np.load(file_path, allow_pickle=True).item()\n",
    "            all_proteins.append(protein)\n",
    "\n",
    "# Now, all_proteins contains the loaded proteins from all folders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming X and y are your feature matrix and target variable, as in the scikit-learn example\n",
    "print(os.getcwd())\n",
    "f = open(\"DeepTMHMM.partitions.json\")\n",
    "\n",
    "labels = json.load(f)\n",
    "print(labels.keys())\n",
    "encoder_path = f\"../encoder_proteins/{cv}/{protein['id']}\"\n",
    "read_dictionary = np.load(encoder_path + \".npy\", allow_pickle='TRUE').item()\n",
    "print(read_dictionary)\n",
    "# Convert data to PyTorch tensors\n",
    "X = torch.tensor(X.values, dtype=torch.float32)\n",
    "y = torch.tensor(y.values, dtype=torch.float32)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create DataLoader for training and testing sets\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseNN(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(DenseNN, self).__init__()\n",
    "        \n",
    "        # Flatten the input to a 1D array\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        # Define fully connected layers\n",
    "        self.fc1 = nn.Linear(input_size, 256)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.relu2 = nn.ReLU()\n",
    "\n",
    "        # Output layer with softmax activation for multiclass classification\n",
    "        self.fc3 = nn.Linear(128, num_classes)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.relu1(self.fc1(x))\n",
    "        x = self.relu2(self.fc2(x))\n",
    "        x = self.softmax(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "# Specify the input size (L * 512) and number of classes (6)\n",
    "input_size = L * 512\n",
    "num_classes = 6\n",
    "\n",
    "# Create the model\n",
    "dense_nn_model = DenseNN(input_size, num_classes)\n",
    "\n",
    "# Print the model architecture\n",
    "print(dense_nn_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = dense_nn_model(input_size, num_classes)\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels.view(-1, 1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Evaluation on the test set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        predictions = (outputs > 0.5).float()\n",
    "        all_predictions.extend(predictions.numpy())\n",
    "        all_labels.extend(labels.numpy())\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(all_labels, all_predictions)\n",
    "print(f\"Accuracy: {accuracy}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rfl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
