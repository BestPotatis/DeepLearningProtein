{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1:\n",
      "Labels shape: torch.Size([3, 446])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andre\\AppData\\Local\\Temp\\ipykernel_8912\\3453191837.py:47: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = [torch.tensor(sample['labels']) for sample in batch]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, pad_sequence\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, folder_path, split):\n",
    "        self.folder_path = folder_path\n",
    "        self.split = split\n",
    "        self.data = self.load_data()\n",
    "        self.label_encoding = {'I': 0, 'O':1, 'P': 2, 'S': 3, 'M':4, 'B': 5}\n",
    "\n",
    "    def load_data(self):\n",
    "        file_list = [f for f in os.listdir(self.folder_path) if f.endswith('.npy')]\n",
    "        file_list.sort()  # Make sure the order is consistent\n",
    "\n",
    "        data = []\n",
    "        for file_name in file_list:\n",
    "            file_path = os.path.join(self.folder_path, file_name)\n",
    "            data.append(np.load(file_path, allow_pickle=True).item())\n",
    "\n",
    "        return data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sample = self.data[index]\n",
    "        #print(sample['data'])\n",
    "        inputs = sample['data']\n",
    "        labels_str = sample['labels']\n",
    "\n",
    "        labels_list = [self.label_encoding[label] for label in labels_str]\n",
    "        labels_tensor = torch.tensor(labels_list, dtype=torch.long)\n",
    "        return {'data': inputs, 'labels': labels_tensor}\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # Sort the batch by sequence length in descending order\n",
    "    batch = sorted(batch, key=lambda x: len(x['data']), reverse=True)\n",
    "\n",
    "    # Pad sequences for data\n",
    "    data = [torch.tensor(sample['data']) for sample in batch]\n",
    "    padded_data = pad_sequence(data, batch_first=True)\n",
    "\n",
    "    # Pad sequences for labels\n",
    "    labels = [torch.tensor(sample['labels']) for sample in batch]\n",
    "    padded_labels = pad_sequence(labels, batch_first=True)\n",
    "\n",
    "    # Pack the padded sequences for data\n",
    "    lengths = [len(seq) for seq in data]\n",
    "    packed_data = pack_padded_sequence(padded_data, lengths=lengths, batch_first=True, enforce_sorted=False)\n",
    "\n",
    "    return {'data': packed_data, 'labels': padded_labels} \n",
    "\n",
    "def create_data_loaders(data_root):\n",
    "    splits = [\n",
    "        (['cv0', 'cv1', 'cv2'], 'cv3' , 'cv4'),\n",
    "        (['cv1', 'cv2', 'cv3',], 'cv4', 'cv0'),\n",
    "        (['cv2', 'cv3', 'cv4'], 'cv0', 'cv1'),\n",
    "        (['cv3', 'cv4', 'cv0'], 'cv1', 'cv2'),\n",
    "        (['cv4', 'cv0', 'cv1'], 'cv2', 'cv3'),\n",
    "    ]\n",
    "\n",
    "    data_loaders = {}\n",
    "\n",
    "    for train_folders, val_folder, test_folder in splits:\n",
    "        train_dataset = [CustomDataset(os.path.join(data_root, folder), 'train') for folder in train_folders]\n",
    "        val_dataset = CustomDataset(os.path.join(data_root, val_folder), 'val')\n",
    "        test_dataset = CustomDataset(os.path.join(data_root, test_folder), 'test')\n",
    "        \n",
    "        for train_folders, train_dataset in zip(train_folders, train_dataset):\n",
    "            data_loaders[train_folders] = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "        data_loaders[val_folder] = DataLoader(val_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "        data_loaders[test_folder] = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    return data_loaders\n",
    "\n",
    "data_root = os.getcwd() + '\\encoder_proteins'\n",
    "data_loaders = create_data_loaders(data_root)\n",
    "\n",
    "# Accessing the data loaders\n",
    "train_loader_cv0 = data_loaders['cv0']\n",
    "val_loader_cv1 = data_loaders['cv1']\n",
    "test_loader_cv2 = data_loaders['cv2']\n",
    "\n",
    "# Iterate through a few batches to test the DataLoader\n",
    "for batch_idx, batch in enumerate(train_loader_cv0):\n",
    "    inputs, labels = batch['data'], batch['labels']\n",
    "\n",
    "    print(f\"Batch {batch_idx + 1}:\")\n",
    "    #print(\"Inputs shape:\", inputs.shape)  # Assuming inputs is a NumPy array or a PyTorch tensor\n",
    "    print(\"Labels shape:\", labels.shape)  # Assuming labels is a NumPy array or a PyTorch tensor\n",
    "\n",
    "    # Break the loop after a few batches for testing purposes\n",
    "    if batch_idx == 3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cv0, cv1, cv2 for train, cv3 for validation, cv4 for test\n",
    "cv1, cv2, cv3 for train, cv4 for validation, cv0 for test\n",
    "cv2, cv3, cv4 for train, cv0 for validation, cv1 for test\n",
    "cv3, cv4, cv0 for train, cv1 for validation, cv2 for test\n",
    "cv4, cv0, cv1 for train, cv2 for validation, cv3 for test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Fs2Kga0AkDaw"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DENSE NN Model** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 252
    },
    "id": "e_9JbrzQVrwh",
    "outputId": "d88909cf-b6e3-4a62-80bd-433d71a5200f"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-e250a7b9a05d>\u001b[0m in \u001b[0;36m<cell line: 26>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# Specify the input size (L * 512) and number of classes (6)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0minput_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mL\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0mnum_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'L' is not defined"
     ]
    }
   ],
   "source": [
    "class DenseNN(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(DenseNN, self).__init__()\n",
    "\n",
    "        # Flatten the input to a 1D array\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        # Define fully connected layers\n",
    "        self.fc1 = nn.Linear(input_size, 256)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.relu2 = nn.ReLU()\n",
    "\n",
    "        # Output layer with softmax activation for multiclass classification\n",
    "        self.fc3 = nn.Linear(128, num_classes)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.relu1(self.fc1(x))\n",
    "        x = self.relu2(self.fc2(x))\n",
    "        x = self.softmax(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "# Specify the input size (L * 512) and number of classes (6)\n",
    "input_size = L * 512\n",
    "num_classes = 6\n",
    "\n",
    "# Create the model\n",
    "dense_nn_model = DenseNN(input_size, num_classes)\n",
    "\n",
    "# Print the model architecture\n",
    "print(dense_nn_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "rUJ4kkLWkNjw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andre\\Documents\\DeepLearning Project\n"
     ]
    }
   ],
   "source": [
    "# Define the base directory where your protein folders are located\n",
    "print(os.getcwd())\n",
    "os.chdir('encoder_proteins') \n",
    "base_dir = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "SP0MmQP0kMNi"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cv0', 'cv1', 'cv2', 'cv3', 'cv4', 'cv5']\n",
      "cv: 0\n",
      "cv: 1\n",
      "cv: 3\n",
      "Train proteins shape:  2\n",
      "Test proteins shape:  1\n",
      "Val proteins shape:  1\n"
     ]
    }
   ],
   "source": [
    "# List to store loaded proteins\n",
    "train_proteins = []\n",
    "test_proteins = []\n",
    "val_proteins = []\n",
    "# Iterate through the folders 'cv0' to 'cv5'\n",
    "print(os.listdir(base_dir))\n",
    "# Train split:\n",
    "for i in range(2):  # Assuming folders are named 'cv0' to 'cv5'\n",
    "    folder_name = f'cv{i}'\n",
    "    print(\"cv:\", i)\n",
    "    folder_path = os.path.join(base_dir, folder_name)\n",
    "    # Iterate through files in the folder\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.npy'):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "\n",
    "            # Load the protein and append to the list\n",
    "            protein = np.load(file_path, allow_pickle=True).item()\n",
    "            train_proteins.append(protein)\n",
    "# Test split:\n",
    "for i in range(1):\n",
    "    folder_name = f'cv{i+3}'\n",
    "    print(\"cv:\", i+3)\n",
    "    folder_path = os.path.join(base_dir, folder_name)\n",
    "\n",
    "    # Iterate through files in the folder\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.npy'):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "\n",
    "            # Load the protein and append to the list\n",
    "            protein = np.load(file_path, allow_pickle=True).item()\n",
    "            test_proteins.append(protein)\n",
    "# Val dataset:\n",
    "\n",
    "folder_name = f'cv{5}'\n",
    "folder_path = os.path.join(base_dir, folder_name)\n",
    "\n",
    "# Iterate through files in the folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.npy'):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "\n",
    "        # Load the protein and append to the list\n",
    "        protein = np.load(file_path, allow_pickle=True).item()\n",
    "        val_proteins.append(protein)\n",
    "\n",
    "print(\"Train proteins shape: \",len(train_proteins))\n",
    "print(\"Test proteins shape: \", len(test_proteins))\n",
    "print(\"Val proteins shape: \", len(val_proteins))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HoCfIUreVk69"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Now, all_proteins contains the loaded proteins from all folders\n",
    "\n",
    "# Assuming X and y are your feature matrix and target variable, as in the scikit-learn example\n",
    "print(os.getcwd())\n",
    "f = open(\"DeepTMHMM.partitions.json\")\n",
    "\n",
    "labels = json.load(f)\n",
    "print(labels.keys())\n",
    "encoder_path = f\"../encoder_proteins/{cv}/{protein['id']}\"\n",
    "read_dictionary = np.load(encoder_path + \".npy\", allow_pickle='TRUE').item()\n",
    "print(read_dictionary)\n",
    "# Convert data to PyTorch tensors\n",
    "X = torch.tensor(X.values, dtype=torch.float32)\n",
    "y = torch.tensor(y.values, dtype=torch.float32)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create DataLoader for training and testing sets\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "input_size = X.shape[1]\n",
    "model = LogisticRegression(input_size)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels.view(-1, 1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Evaluation on the test set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        predictions = (outputs > 0.5).float()\n",
    "        all_predictions.extend(predictions.numpy())\n",
    "        all_labels.extend(labels.numpy())\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(all_labels, all_predictions)\n",
    "print(f\"Accuracy: {accuracy}\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
