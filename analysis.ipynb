{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import cycle, islice\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, pad_sequence\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "from classifier_rnn import RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, folder_path, split):\n",
    "        self.folder_path = folder_path\n",
    "        self.split = split\n",
    "        self.data = self.load_data()\n",
    "        \n",
    "        self.label_encoding = {'I': 0, 'O': 1, 'P': 2, 'S': 3, 'M': 4, 'B': 5}\n",
    "        self.one_hot = LabelBinarizer()\n",
    "        self.one_hot.fit(list(self.label_encoding.values()))\n",
    "\n",
    "    def load_data(self):\n",
    "        file_list = [f for f in os.listdir(self.folder_path) if f.endswith('.npy')]\n",
    "        file_list.sort()  # Make sure the order is consistent\n",
    "\n",
    "        data = []\n",
    "        for file_name in file_list:\n",
    "            file_path = os.path.join(self.folder_path, file_name)\n",
    "            data.append(np.load(file_path, allow_pickle=True).item())\n",
    "\n",
    "        return data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sample = self.data[index]\n",
    "        #print(sample['data'])\n",
    "        inputs = sample['data']\n",
    "        labels_str = sample['labels']\n",
    "\n",
    "        labels_list = [self.label_encoding[label] for label in labels_str]\n",
    "        labels_list = self.one_hot.transform(labels_list)\n",
    "        \n",
    "        labels_tensor = torch.tensor(labels_list, dtype=torch.float)\n",
    "\n",
    "        return {'data': inputs, 'labels': labels_tensor}\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # sort the batch by sequence length in descending order\n",
    "    batch = sorted(batch, key=lambda x: len(x['data']), reverse=True)\n",
    "    \n",
    "    # pad sequences for data\n",
    "    data = [torch.tensor(sample['data']) for sample in batch]\n",
    "    padded_data = pad_sequence(data, batch_first=True)\n",
    "\n",
    "    # Pad sequences for labels\n",
    "    labels = [torch.tensor(sample['labels']) for sample in batch]\n",
    "    padded_labels = pad_sequence(labels, batch_first=True)\n",
    "    \n",
    "    # Pack the padded sequences for data\n",
    "    lengths = [len(seq) for seq in data]\n",
    "    #packed_data = pack_padded_sequence(padded_data, lengths=lengths, batch_first=True, enforce_sorted=True)\n",
    "\n",
    "    return {'data': padded_data, 'labels': padded_labels, \"lengths\": lengths} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD 1\n",
      "--------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zs/b0y2x66j1kx9mlgj_2cpj7bw0000gp/T/ipykernel_6246/2825261995.py:47: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = [torch.tensor(sample['labels']) for sample in batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(16.1144, grad_fn=<AddBackward0>)\n",
      "tensor(13.0598, grad_fn=<AddBackward0>)\n",
      "tensor(11.0948, grad_fn=<AddBackward0>)\n",
      "tensor(10.8192, grad_fn=<AddBackward0>)\n",
      "tensor(10.6535, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'detach'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/helenakeitum/Desktop/DeepLearningProtein/analysis.ipynb Cell 3\u001b[0m line \u001b[0;36m8\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/helenakeitum/Desktop/DeepLearningProtein/analysis.ipynb#W2sZmlsZQ%3D%3D?line=83'>84</a>\u001b[0m         \u001b[39mif\u001b[39;00m epoch \u001b[39m%\u001b[39m \u001b[39m10\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/helenakeitum/Desktop/DeepLearningProtein/analysis.ipynb#W2sZmlsZQ%3D%3D?line=84'>85</a>\u001b[0m             \u001b[39mprint\u001b[39m(train_loss[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/helenakeitum/Desktop/DeepLearningProtein/analysis.ipynb#W2sZmlsZQ%3D%3D?line=86'>87</a>\u001b[0m     gen_train_loss\u001b[39m.\u001b[39mappend(np\u001b[39m.\u001b[39mmean(train_loss\u001b[39m.\u001b[39;49mdetach\u001b[39m.\u001b[39mnumpy()))        \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/helenakeitum/Desktop/DeepLearningProtein/analysis.ipynb#W2sZmlsZQ%3D%3D?line=88'>89</a>\u001b[0m \u001b[39mprint\u001b[39m(gen_train_loss)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'detach'"
     ]
    }
   ],
   "source": [
    "# config\n",
    "k_folds = 5\n",
    "num_epochs = 50\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "lr = 1e-3\n",
    "\n",
    "# define models to be analyzed\n",
    "model_rnn = RNN(512, 300, 6)\n",
    "optimizer = optim.Adam(model_rnn.parameters(), lr = lr)\n",
    "\n",
    "# set fixed random number seed\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# create cv's corresponding to deeptmhmm's\n",
    "cvs = list(range(5))\n",
    "kfolds = []\n",
    "for idx, split in enumerate(range(5)):\n",
    "    \n",
    "    # make cycling list and define train/val/test splits\n",
    "    idxs = np.asarray(list(islice(cycle(cvs), idx, idx + 5)))\n",
    "    train_idx, val_idx, test_idx = idxs[:3], idxs[3], idxs[4]\n",
    "    \n",
    "    kfolds.append((train_idx, val_idx, test_idx))\n",
    "\n",
    "# make on big concatenated dataset of all splits\n",
    "data_cvs = np.squeeze([CustomDataset(os.path.join(\"encoder_proteins_test\", folder), 'train') for folder in ['cv0', 'cv1', 'cv2', 'cv3' , 'cv4']])\n",
    "\n",
    "gen_train_loss = []\n",
    "gen_acc = []\n",
    "\n",
    "# k-fold cross validation\n",
    "for fold, (train_ids, val_id, test_id) in enumerate(kfolds):    \n",
    "    print(f'FOLD {fold + 1}')\n",
    "    print('--------------------------------')\n",
    "    \n",
    "    # concatenates the data from the different cv's\n",
    "    training_data = np.concatenate(data_cvs[train_ids], axis = 0)\n",
    "    \n",
    "    # define data loaders for train/val/test data in this fold (collate 0 pads for same-length)\n",
    "    trainloader = DataLoader(\n",
    "                        training_data, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "    valloader = DataLoader(data_cvs[val_id])\n",
    "    testloader = DataLoader(data_cvs[test_id])\n",
    "    \n",
    "    # training\n",
    "    step = 0\n",
    "    model_rnn.train()\n",
    "\n",
    "    train_loss = []\n",
    "    train_accuracies = []\n",
    "    \n",
    "    valid_loss = []\n",
    "    valid_accuracies = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        for batch_idx, batch in enumerate(trainloader):\n",
    "            inputs, labels, lengths = batch['data'], batch['labels'], batch['lengths']\n",
    "\n",
    "            # receive output from rnn\n",
    "            output = model_rnn(inputs)  \n",
    "\n",
    "            loss = 0\n",
    "            for l in range(output.shape[0]):\n",
    "                # masking the zero-padded outputs\n",
    "                batch_output = output[l][:lengths[l]]\n",
    "                batch_labels = labels[l][:lengths[l]]\n",
    "                \n",
    "                # compute cross-entropy loss\n",
    "                loss += loss_function(batch_output, batch_labels)\n",
    "                \n",
    "            train_loss.append(loss)\n",
    "            \n",
    "            # gradient update\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()  \n",
    "            \n",
    "            step += 1\n",
    "            \n",
    "            predictions = output.max(-1)[1]\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            print(train_loss[-1])\n",
    "        \n",
    "    #gen_train_loss.append(np.mean(train_loss.detach.numpy()))        \n",
    "\n",
    "#print(gen_train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_exam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
