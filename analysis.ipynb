{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import cycle, islice\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import json\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, pad_sequence\n",
    "from collections import defaultdict\n",
    "\n",
    "from classifier_rnn import RNN\n",
    "from accuracy import test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, folder_path, split):\n",
    "        self.folder_path = folder_path\n",
    "        self.split = split\n",
    "        self.data = self.load_data()\n",
    "        \n",
    "        self.label_encoding = {'I': 0, 'O': 1, 'P': 2, 'S': 3, 'M': 4, 'B': 5}\n",
    "\n",
    "    def load_data(self):\n",
    "        file_list = [f for f in os.listdir(self.folder_path) if f.endswith('.npy')]\n",
    "        file_list.sort()  # Make sure the order is consistent\n",
    "\n",
    "        data = []\n",
    "        for file_name in file_list:\n",
    "            file_path = os.path.join(self.folder_path, file_name)\n",
    "            data.append(np.load(file_path, allow_pickle=True).item())\n",
    "\n",
    "        return data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sample = self.data[index]\n",
    "        inputs = sample['data']\n",
    "        labels_str = sample['labels']\n",
    "\n",
    "        labels_list = [self.label_encoding[label] for label in labels_str]\n",
    "        labels_tensor = torch.tensor(labels_list, dtype=torch.long)\n",
    "\n",
    "        return {'data': inputs, 'labels': labels_tensor}\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # sort the batch by sequence length in descending order\n",
    "    batch = sorted(batch, key=lambda x: len(x['data']), reverse=True)\n",
    "    \n",
    "    # pad sequences for data\n",
    "    data = [torch.tensor(sample['data']) for sample in batch]\n",
    "    padded_data = pad_sequence(data, batch_first=True)\n",
    "\n",
    "    # Pad sequences for labels\n",
    "    labels = [torch.tensor(sample['labels']) for sample in batch]\n",
    "    padded_labels = pad_sequence(labels, batch_first=True)\n",
    "    \n",
    "    # Pack the padded sequences for data\n",
    "    lengths = [len(seq) for seq in data]\n",
    "    #packed_data = pack_padded_sequence(padded_data, lengths=lengths, batch_first=True, enforce_sorted=True)\n",
    "\n",
    "    return {'data': padded_data, 'labels': padded_labels, \"lengths\": lengths} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nn(model, trainloader, valloader, loss_function, optimizer, fold, experiment_file_path, device, num_epochs = 50):\n",
    "\n",
    "    model.train()\n",
    "    \n",
    "    train_loss = []\n",
    "    valid_loss = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        for _, batch in enumerate(trainloader):\n",
    "            inputs, labels, lengths = batch['data'], batch['labels'], torch.tensor(batch['lengths'])\n",
    "            inputs, labels, lengths = inputs.to(device), labels.to(device), lengths.to(device)\n",
    "            \n",
    "            # receive output from rnn\n",
    "            output = model(inputs)  \n",
    "            \n",
    "            loss = 0\n",
    "            for l in range(output.shape[0]):\n",
    "                # masking the zero-padded outputs\n",
    "                batch_output = output[l][:lengths[l]]\n",
    "                batch_labels = labels[l][:lengths[l]]\n",
    "                \n",
    "                # compute cross-entropy loss\n",
    "                loss += loss_function(batch_output, batch_labels) / trainloader.batch_size\n",
    "                \n",
    "            # gradient update\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()  \n",
    "        \n",
    "        train_loss_epoch = test_predictions(model = model, loader = trainloader, loss_function = loss_function,\n",
    "                         cv = str(fold), experiment_file_path = experiment_file_path, condition = \"train\", epoch = str(epoch),\n",
    "                         device = device)   \n",
    "\n",
    "        valid_loss_epoch = test_predictions(model = model, loader = valloader, loss_function = loss_function,\n",
    "                        cv = str(fold), experiment_file_path = experiment_file_path, condition = \"val\", epoch = str(epoch),\n",
    "                        device = device)\n",
    "        \n",
    "        # # receive mean loss for this epoch\n",
    "        train_loss.append(train_loss_epoch)\n",
    "        valid_loss.append(valid_loss_epoch)\n",
    "        \n",
    "        # if epoch % 10 == 0:\n",
    "        #     print(\"training loss: \", train_loss[-1], \\\n",
    "        #         \"\\t validation loss: \", valid_loss[-1])\n",
    "    \n",
    "    return train_loss, valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "\n",
      "HIDDEN_SIZE 64\n",
      "--------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zs/b0y2x66j1kx9mlgj_2cpj7bw0000gp/T/ipykernel_34231/4098633305.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = [torch.tensor(sample['labels']) for sample in batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss:  15.625606536865234 \t validation loss:  5.232363224029541\n",
      "\n",
      "HIDDEN_SIZE 128\n",
      "--------------------------------\n",
      "training loss:  15.45163631439209 \t validation loss:  5.108148574829102\n",
      "\n",
      "HIDDEN_SIZE 256\n",
      "--------------------------------\n",
      "training loss:  15.007866859436035 \t validation loss:  4.91313362121582\n",
      "\n",
      "HIDDEN_SIZE 512\n",
      "--------------------------------\n",
      "training loss:  14.538534164428711 \t validation loss:  4.7536211013793945\n",
      "\n",
      "best params for fold 1:  512\n",
      "test loss for fold 1:  2.8529481887817383\n",
      "\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "\n",
      "HIDDEN_SIZE 64\n",
      "--------------------------------\n",
      "training loss:  15.499673843383789 \t validation loss:  5.212826728820801\n",
      "\n",
      "HIDDEN_SIZE 128\n",
      "--------------------------------\n",
      "training loss:  15.143696784973145 \t validation loss:  4.933631896972656\n",
      "\n",
      "HIDDEN_SIZE 256\n",
      "--------------------------------\n",
      "training loss:  14.762297630310059 \t validation loss:  4.826035022735596\n",
      "\n",
      "HIDDEN_SIZE 512\n",
      "--------------------------------\n",
      "training loss:  14.067039489746094 \t validation loss:  4.538074493408203\n",
      "\n",
      "best params for fold 2:  512\n",
      "test loss for fold 2:  5.341287136077881\n",
      "\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "\n",
      "HIDDEN_SIZE 64\n",
      "--------------------------------\n",
      "training loss:  15.672155380249023 \t validation loss:  5.257295608520508\n",
      "\n",
      "HIDDEN_SIZE 128\n",
      "--------------------------------\n",
      "training loss:  14.984122276306152 \t validation loss:  5.083629608154297\n",
      "\n",
      "HIDDEN_SIZE 256\n",
      "--------------------------------\n",
      "training loss:  14.974287033081055 \t validation loss:  5.144260883331299\n",
      "\n",
      "HIDDEN_SIZE 512\n",
      "--------------------------------\n",
      "training loss:  13.77003288269043 \t validation loss:  4.912364482879639\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/helenakeitum/Desktop/DeepLearningProtein/analysis.ipynb Cell 4\u001b[0m line \u001b[0;36m7\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/helenakeitum/Desktop/DeepLearningProtein/analysis.ipynb#X23sZmlsZQ%3D%3D?line=73'>74</a>\u001b[0m optimizer \u001b[39m=\u001b[39m optim\u001b[39m.\u001b[39mAdam(model_rnn\u001b[39m.\u001b[39mparameters(), lr \u001b[39m=\u001b[39m lr)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/helenakeitum/Desktop/DeepLearningProtein/analysis.ipynb#X23sZmlsZQ%3D%3D?line=75'>76</a>\u001b[0m \u001b[39m# train and validate model\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/helenakeitum/Desktop/DeepLearningProtein/analysis.ipynb#X23sZmlsZQ%3D%3D?line=76'>77</a>\u001b[0m train_loss, valid_loss \u001b[39m=\u001b[39m train_nn(model \u001b[39m=\u001b[39;49m model_rnn, \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/helenakeitum/Desktop/DeepLearningProtein/analysis.ipynb#X23sZmlsZQ%3D%3D?line=77'>78</a>\u001b[0m                                   trainloader \u001b[39m=\u001b[39;49m trainloader, valloader \u001b[39m=\u001b[39;49m valloader,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/helenakeitum/Desktop/DeepLearningProtein/analysis.ipynb#X23sZmlsZQ%3D%3D?line=78'>79</a>\u001b[0m                                   loss_function \u001b[39m=\u001b[39;49m loss_function, optimizer \u001b[39m=\u001b[39;49m optimizer, \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/helenakeitum/Desktop/DeepLearningProtein/analysis.ipynb#X23sZmlsZQ%3D%3D?line=79'>80</a>\u001b[0m                                   fold \u001b[39m=\u001b[39;49m fold, experiment_file_path \u001b[39m=\u001b[39;49m experiment_file_path, \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/helenakeitum/Desktop/DeepLearningProtein/analysis.ipynb#X23sZmlsZQ%3D%3D?line=80'>81</a>\u001b[0m                                   device \u001b[39m=\u001b[39;49m device, num_epochs \u001b[39m=\u001b[39;49m num_epochs)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/helenakeitum/Desktop/DeepLearningProtein/analysis.ipynb#X23sZmlsZQ%3D%3D?line=82'>83</a>\u001b[0m \u001b[39m# save models and losses\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/helenakeitum/Desktop/DeepLearningProtein/analysis.ipynb#X23sZmlsZQ%3D%3D?line=83'>84</a>\u001b[0m param_models\u001b[39m.\u001b[39mappend(model_rnn)   \n",
      "\u001b[1;32m/Users/helenakeitum/Desktop/DeepLearningProtein/analysis.ipynb Cell 4\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/helenakeitum/Desktop/DeepLearningProtein/analysis.ipynb#X23sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mstep()  \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/helenakeitum/Desktop/DeepLearningProtein/analysis.ipynb#X23sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m train_loss_epoch \u001b[39m=\u001b[39m test_predictions(model \u001b[39m=\u001b[39m model, loader \u001b[39m=\u001b[39m trainloader, loss_function \u001b[39m=\u001b[39m loss_function,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/helenakeitum/Desktop/DeepLearningProtein/analysis.ipynb#X23sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m                  cv \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(fold), experiment_file_path \u001b[39m=\u001b[39m experiment_file_path, condition \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m, epoch \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(epoch),\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/helenakeitum/Desktop/DeepLearningProtein/analysis.ipynb#X23sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m                  device \u001b[39m=\u001b[39m device)   \n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/helenakeitum/Desktop/DeepLearningProtein/analysis.ipynb#X23sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m valid_loss_epoch \u001b[39m=\u001b[39m test_predictions(model \u001b[39m=\u001b[39;49m model, loader \u001b[39m=\u001b[39;49m valloader, loss_function \u001b[39m=\u001b[39;49m loss_function,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/helenakeitum/Desktop/DeepLearningProtein/analysis.ipynb#X23sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m                 cv \u001b[39m=\u001b[39;49m \u001b[39mstr\u001b[39;49m(fold), experiment_file_path \u001b[39m=\u001b[39;49m experiment_file_path, condition \u001b[39m=\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mval\u001b[39;49m\u001b[39m\"\u001b[39;49m, epoch \u001b[39m=\u001b[39;49m \u001b[39mstr\u001b[39;49m(epoch),\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/helenakeitum/Desktop/DeepLearningProtein/analysis.ipynb#X23sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m                 device \u001b[39m=\u001b[39;49m device)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/helenakeitum/Desktop/DeepLearningProtein/analysis.ipynb#X23sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m \u001b[39m# # receive mean loss for this epoch\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/helenakeitum/Desktop/DeepLearningProtein/analysis.ipynb#X23sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m train_loss\u001b[39m.\u001b[39mappend(train_loss_epoch)\n",
      "File \u001b[0;32m~/Desktop/DeepLearningProtein/accuracy.py:191\u001b[0m, in \u001b[0;36mtest_predictions\u001b[0;34m(model, loader, loss_function, cv, experiment_file_path, device, condition, epoch)\u001b[0m\n\u001b[1;32m    188\u001b[0m inputs, labels, lengths \u001b[39m=\u001b[39m batch[\u001b[39m'\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m'\u001b[39m], batch[\u001b[39m'\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m'\u001b[39m], torch\u001b[39m.\u001b[39mtensor(batch[\u001b[39m'\u001b[39m\u001b[39mlengths\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m    189\u001b[0m inputs, labels, lengths \u001b[39m=\u001b[39m inputs\u001b[39m.\u001b[39mto(device), labels\u001b[39m.\u001b[39mto(device), lengths\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m--> 191\u001b[0m output \u001b[39m=\u001b[39m model(inputs)\n\u001b[1;32m    193\u001b[0m predict_label \u001b[39m=\u001b[39m []\n\u001b[1;32m    194\u001b[0m predict_prot_type \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/miniconda3/envs/dl_exam/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/dl_exam/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/DeepLearningProtein/classifier_rnn.py:18\u001b[0m, in \u001b[0;36mRNN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m     17\u001b[0m     \u001b[39m# return output and last hidden state\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m     x, (h, c) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlstm(x)\n\u001b[1;32m     20\u001b[0m     \u001b[39m# fully-connected output layer\u001b[39;00m\n\u001b[1;32m     21\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlinear(x)\n",
      "File \u001b[0;32m~/miniconda3/envs/dl_exam/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/dl_exam/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/dl_exam/lib/python3.9/site-packages/torch/nn/modules/rnn.py:879\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    876\u001b[0m         hx \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[1;32m    878\u001b[0m \u001b[39mif\u001b[39;00m batch_sizes \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 879\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39;49mlstm(\u001b[39minput\u001b[39;49m, hx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_flat_weights, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_layers,\n\u001b[1;32m    880\u001b[0m                       \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbidirectional, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_first)\n\u001b[1;32m    881\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    882\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39mlstm(\u001b[39minput\u001b[39m, batch_sizes, hx, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flat_weights, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias,\n\u001b[1;32m    883\u001b[0m                       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_layers, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbidirectional)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# set device to gpu if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.set_default_device(\"cuda:0\")\n",
    "\n",
    "# config\n",
    "k_folds = 5\n",
    "num_epochs = 100\n",
    "batch_size = 32\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "lr = 1e-3\n",
    "tuning = [64, 128, 256, 512]\n",
    "\n",
    "experiment_file_list = []\n",
    "for i in tuning:\n",
    "    experiment_file_list.append(f\"stat_data_{i}\")\n",
    "    experiment_json = {}\n",
    "    open(experiment_file_list[-1], 'w').write(json.dumps(experiment_json))\n",
    "\n",
    "# set fixed random number seed\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# create cv's corresponding to deeptmhmm's\n",
    "cvs = list(range(5))\n",
    "kfolds = []\n",
    "for idx, split in enumerate(range(5)):\n",
    "    \n",
    "    # make cycling list and define train/val/test splits\n",
    "    idxs = np.asarray(list(islice(cycle(cvs), idx, idx + 5)))\n",
    "    train_idx, val_idx, test_idx = idxs[:3], idxs[3], idxs[4]\n",
    "    \n",
    "    kfolds.append((train_idx, val_idx, test_idx))\n",
    "\n",
    "# make on big concatenated dataset of all splits\n",
    "data_cvs = np.squeeze([CustomDataset(os.path.join(\"encoder_proteins_test\", folder), 'train') for folder in ['cv0', 'cv1', 'cv2', 'cv3' , 'cv4']])\n",
    "\n",
    "gen_train_loss = np.zeros((len(kfolds), num_epochs))\n",
    "gen_train_acc = np.zeros((len(kfolds), num_epochs))\n",
    "fold_test_loss = np.zeros((len(kfolds)))\n",
    "fold_test_acc = np.zeros((len(kfolds)))\n",
    "\n",
    "# k-fold cross validation\n",
    "for fold, (train_ids, val_id, test_id) in enumerate(kfolds):    \n",
    "    # print(f'\\nFOLD {fold + 1}')\n",
    "    # print('--------------------------------')\n",
    "    \n",
    "    # concatenates the data from the different cv's\n",
    "    training_data = np.concatenate(data_cvs[train_ids], axis = 0)\n",
    "    \n",
    "    # define data loaders for train/val/test data in this fold (collate 0 pads for same-length)\n",
    "    trainloader = DataLoader(\n",
    "                        training_data, batch_size=batch_size, shuffle=True, collate_fn=collate_fn, drop_last = True)\n",
    "\n",
    "    valloader = DataLoader(data_cvs[val_id], batch_size=batch_size, shuffle=False, collate_fn=collate_fn, drop_last = True)\n",
    "    testloader = DataLoader(data_cvs[test_id], batch_size=batch_size, shuffle=False, collate_fn=collate_fn, drop_last = True)\n",
    "    \n",
    "    param_models = []\n",
    "    \n",
    "    train_loss_param = np.zeros((len(tuning), num_epochs))\n",
    "    val_loss_param = np.zeros((len(tuning), num_epochs))\n",
    "    \n",
    "    # hyperparameter tune\n",
    "    for idx, param in enumerate(tuning):\n",
    "        experiment_file_path = experiment_file_list[idx] \n",
    "        \n",
    "        # print(f'\\nHIDDEN_SIZE {param}')\n",
    "        # print('--------------------------------')\n",
    "        \n",
    "        # define models to be analyzed\n",
    "        model_rnn = RNN(512, param, 6)\n",
    "        model_rnn.to(device)\n",
    "        \n",
    "        optimizer = optim.Adam(model_rnn.parameters(), lr = lr)\n",
    "        \n",
    "        # train and validate model\n",
    "        train_loss, valid_loss = train_nn(model = model_rnn, \n",
    "                                        trainloader = trainloader, valloader = valloader,\n",
    "                                        loss_function = loss_function, optimizer = optimizer, \n",
    "                                        fold = fold, experiment_file_path = experiment_file_path, \n",
    "                                        device = device, num_epochs = num_epochs)\n",
    "        \n",
    "        # save models and losses\n",
    "        param_models.append(model_rnn)   \n",
    "        train_loss_param[idx] = train_loss\n",
    "        val_loss_param[idx] = valid_loss\n",
    "    \n",
    "    # test for the best model\n",
    "    best_param_idx = val_loss_param[:, -1].argmin()\n",
    "    best_model = param_models[best_param_idx]\n",
    "    experiment_file_path = experiment_file_list[best_param_idx]\n",
    "    \n",
    "    # print(f\"\\nbest params for fold {fold + 1}: \", tuning[best_param_idx])  \n",
    "    \n",
    "    test_loss = test_predictions(model = best_model,\n",
    "                    loader = testloader,\n",
    "                    loss_function = loss_function,\n",
    "                    cv = str(fold), experiment_file_path = experiment_file_path,\n",
    "                    device = device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_exam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
