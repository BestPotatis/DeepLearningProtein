{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import cycle, islice\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, pad_sequence\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "from classifier_rnn import RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, folder_path, split):\n",
    "        self.folder_path = folder_path\n",
    "        self.split = split\n",
    "        self.data = self.load_data()\n",
    "        \n",
    "        self.label_encoding = {'I': 0, 'O': 1, 'P': 2, 'S': 3, 'M': 4, 'B': 5}\n",
    "        self.one_hot = LabelBinarizer()\n",
    "        self.one_hot.fit(list(self.label_encoding.values()))\n",
    "\n",
    "    def load_data(self):\n",
    "        file_list = [f for f in os.listdir(self.folder_path) if f.endswith('.npy')]\n",
    "        file_list.sort()  # Make sure the order is consistent\n",
    "\n",
    "        data = []\n",
    "        for file_name in file_list:\n",
    "            file_path = os.path.join(self.folder_path, file_name)\n",
    "            data.append(np.load(file_path, allow_pickle=True).item())\n",
    "\n",
    "        return data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sample = self.data[index]\n",
    "        #print(sample['data'])\n",
    "        inputs = sample['data']\n",
    "        labels_str = sample['labels']\n",
    "\n",
    "        labels_list = [self.label_encoding[label] for label in labels_str]\n",
    "        labels_list = self.one_hot.transform(labels_list)\n",
    "        \n",
    "        labels_tensor = torch.tensor(labels_list, dtype=torch.float)\n",
    "\n",
    "        return {'data': inputs, 'labels': labels_tensor}\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # sort the batch by sequence length in descending order\n",
    "    batch = sorted(batch, key=lambda x: len(x['data']), reverse=True)\n",
    "    \n",
    "    # pad sequences for data\n",
    "    data = [torch.tensor(sample['data']) for sample in batch]\n",
    "    padded_data = pad_sequence(data, batch_first=True)\n",
    "\n",
    "    # Pad sequences for labels\n",
    "    labels = [torch.tensor(sample['labels']) for sample in batch]\n",
    "    padded_labels = pad_sequence(labels, batch_first=True)\n",
    "    \n",
    "    # Pack the padded sequences for data\n",
    "    lengths = [len(seq) for seq in data]\n",
    "    #packed_data = pack_padded_sequence(padded_data, lengths=lengths, batch_first=True, enforce_sorted=True)\n",
    "\n",
    "    return {'data': padded_data, 'labels': padded_labels, \"lengths\": lengths} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nn(model, trainloader, valloader, loss_function, optimizer, num_epochs = 50, val_step = 100):\n",
    "    # training\n",
    "    step = 0\n",
    "    model.train()\n",
    "    \n",
    "    train_loss = []\n",
    "    train_accuracies = []\n",
    "    valid_loss = []\n",
    "    valid_accuracies = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        train_batch_loss = []\n",
    "        train_batch_accuracies = []\n",
    "        val_batch_loss = []\n",
    "        val_batch_accuracies = []\n",
    "        \n",
    "        for batch_idx, batch in enumerate(trainloader):\n",
    "            inputs, labels, lengths = batch['data'], batch['labels'], batch['lengths']\n",
    "\n",
    "            # receive output from rnn\n",
    "            output = model(inputs)  \n",
    "\n",
    "            loss = 0\n",
    "            for l in range(output.shape[0]):\n",
    "                # masking the zero-padded outputs\n",
    "                batch_output = output[l][:lengths[l]]\n",
    "                batch_labels = labels[l][:lengths[l]]\n",
    "                \n",
    "                # compute cross-entropy loss\n",
    "                loss += loss_function(batch_output, batch_labels)\n",
    "            \n",
    "            # receive final loss from current batch    \n",
    "            train_batch_loss.append(loss.item())\n",
    "            \n",
    "            # gradient update\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()  \n",
    "            \n",
    "            step += 1\n",
    "            \n",
    "            predictions = output.max(-1)[1]\n",
    "            \n",
    "            # consider only doing this once done with training (after epoch loop)\n",
    "            if step % val_step == 0:\n",
    "                # validate current model with hidden_size = param\n",
    "                with torch.no_grad():\n",
    "                    model.eval()\n",
    "                    \n",
    "                    for batch_idx, batch in enumerate(valloader):\n",
    "                        inputs, labels, lengths = batch['data'], batch['labels'], batch['lengths']\n",
    "                        output = model(inputs)\n",
    "                        \n",
    "                        loss = 0\n",
    "                        for l in range(output.shape[0]):\n",
    "                            # masking the zero-padded outputs\n",
    "                            batch_output = output[l][:lengths[l]]\n",
    "                            batch_labels = labels[l][:lengths[l]]\n",
    "                            \n",
    "                            # compute cross-entropy loss\n",
    "                            loss += loss_function(batch_output, batch_labels)\n",
    "                        \n",
    "                        # receive validation loss from current batch\n",
    "                        val_batch_loss.append(loss.item())\n",
    "                        \n",
    "                    model.train()\n",
    "                \n",
    "        # receive mean loss for this epoch\n",
    "        train_loss.append(np.mean(train_batch_loss))\n",
    "        valid_loss.append(np.mean(val_batch_loss))\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            print(\"training loss: \", train_loss[-1], \\\n",
    "                \"\\t validation loss: \", valid_loss[-1])\n",
    "\n",
    "    return train_loss, train_accuracies, valid_loss, valid_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD 1\n",
      "--------------------------------\n",
      "\n",
      " HIDDEN_SIZE 100\n",
      "--------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zs/b0y2x66j1kx9mlgj_2cpj7bw0000gp/T/ipykernel_6246/2825261995.py:47: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = [torch.tensor(sample['labels']) for sample in batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss:  16.064556121826172 \t validation loss:  5.319646835327148\n",
      "\n",
      " HIDDEN_SIZE 200\n",
      "--------------------------------\n",
      "training loss:  16.12388038635254 \t validation loss:  5.293364524841309\n",
      "\n",
      " HIDDEN_SIZE 300\n",
      "--------------------------------\n",
      "training loss:  16.12946128845215 \t validation loss:  5.272335529327393\n",
      "\n",
      " HIDDEN_SIZE 400\n",
      "--------------------------------\n",
      "training loss:  16.076438903808594 \t validation loss:  5.234174728393555\n",
      "\n",
      " best params for fold 1:  400\n",
      "test loss for fold 1:  4.4634504318237305\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "\n",
      " HIDDEN_SIZE 100\n",
      "--------------------------------\n",
      "training loss:  16.10041618347168 \t validation loss:  5.290262222290039\n",
      "\n",
      " HIDDEN_SIZE 200\n",
      "--------------------------------\n",
      "training loss:  16.17787742614746 \t validation loss:  5.324727535247803\n",
      "\n",
      " HIDDEN_SIZE 300\n",
      "--------------------------------\n",
      "training loss:  16.160429000854492 \t validation loss:  5.298161029815674\n",
      "\n",
      " HIDDEN_SIZE 400\n",
      "--------------------------------\n",
      "training loss:  16.105073928833008 \t validation loss:  5.2188801765441895\n",
      "\n",
      " best params for fold 2:  400\n",
      "test loss for fold 2:  4.668509483337402\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "\n",
      " HIDDEN_SIZE 100\n",
      "--------------------------------\n",
      "training loss:  16.143064498901367 \t validation loss:  5.343156337738037\n",
      "\n",
      " HIDDEN_SIZE 200\n",
      "--------------------------------\n",
      "training loss:  16.112812042236328 \t validation loss:  5.330522060394287\n",
      "\n",
      " HIDDEN_SIZE 300\n",
      "--------------------------------\n",
      "training loss:  16.11011505126953 \t validation loss:  5.29101037979126\n",
      "\n",
      " HIDDEN_SIZE 400\n",
      "--------------------------------\n",
      "training loss:  16.143741607666016 \t validation loss:  5.297079563140869\n",
      "\n",
      " best params for fold 3:  300\n",
      "test loss for fold 3:  5.0836076736450195\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "\n",
      " HIDDEN_SIZE 100\n",
      "--------------------------------\n",
      "training loss:  16.174837112426758 \t validation loss:  5.361177444458008\n",
      "\n",
      " HIDDEN_SIZE 200\n",
      "--------------------------------\n",
      "training loss:  16.139713287353516 \t validation loss:  5.343419551849365\n",
      "\n",
      " HIDDEN_SIZE 300\n",
      "--------------------------------\n",
      "training loss:  16.092117309570312 \t validation loss:  5.3200883865356445\n",
      "\n",
      " HIDDEN_SIZE 400\n",
      "--------------------------------\n",
      "training loss:  16.138797760009766 \t validation loss:  5.338517189025879\n",
      "\n",
      " best params for fold 4:  300\n",
      "test loss for fold 4:  5.034191608428955\n",
      "FOLD 5\n",
      "--------------------------------\n",
      "\n",
      " HIDDEN_SIZE 100\n",
      "--------------------------------\n",
      "training loss:  16.188810348510742 \t validation loss:  5.353604316711426\n",
      "\n",
      " HIDDEN_SIZE 200\n",
      "--------------------------------\n",
      "training loss:  16.085050582885742 \t validation loss:  5.330199718475342\n",
      "\n",
      " HIDDEN_SIZE 300\n",
      "--------------------------------\n",
      "training loss:  16.14836883544922 \t validation loss:  5.328766345977783\n",
      "\n",
      " HIDDEN_SIZE 400\n",
      "--------------------------------\n",
      "training loss:  16.109670639038086 \t validation loss:  5.302078723907471\n",
      "\n",
      " best params for fold 5:  400\n",
      "test loss for fold 5:  4.273497581481934\n",
      "\n",
      " generalization test loss:  4.704651355743408\n"
     ]
    }
   ],
   "source": [
    "# config\n",
    "k_folds = 5\n",
    "num_epochs = 5\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "lr = 1e-3\n",
    "tuning = [100, 200, 300, 400]\n",
    "val_step = 1\n",
    "\n",
    "# set fixed random number seed\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# create cv's corresponding to deeptmhmm's\n",
    "cvs = list(range(5))\n",
    "kfolds = []\n",
    "for idx, split in enumerate(range(5)):\n",
    "    \n",
    "    # make cycling list and define train/val/test splits\n",
    "    idxs = np.asarray(list(islice(cycle(cvs), idx, idx + 5)))\n",
    "    train_idx, val_idx, test_idx = idxs[:3], idxs[3], idxs[4]\n",
    "    \n",
    "    kfolds.append((train_idx, val_idx, test_idx))\n",
    "\n",
    "# make on big concatenated dataset of all splits\n",
    "data_cvs = np.squeeze([CustomDataset(os.path.join(\"encoder_proteins_test\", folder), 'train') for folder in ['cv0', 'cv1', 'cv2', 'cv3' , 'cv4']])\n",
    "\n",
    "gen_train_loss = np.zeros((len(kfolds), num_epochs))\n",
    "gen_train_acc = np.zeros((len(kfolds), num_epochs))\n",
    "fold_test_loss = np.zeros((len(kfolds)))\n",
    "fold_test_acc = np.zeros((len(kfolds)))\n",
    "\n",
    "# k-fold cross validation\n",
    "for fold, (train_ids, val_id, test_id) in enumerate(kfolds):    \n",
    "    print(f'FOLD {fold + 1}')\n",
    "    print('--------------------------------')\n",
    "    \n",
    "    # concatenates the data from the different cv's\n",
    "    training_data = np.concatenate(data_cvs[train_ids], axis = 0)\n",
    "    \n",
    "    # define data loaders for train/val/test data in this fold (collate 0 pads for same-length)\n",
    "    trainloader = DataLoader(\n",
    "                        training_data, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "    valloader = DataLoader(data_cvs[val_id], batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "    testloader = DataLoader(data_cvs[test_id], batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "    \n",
    "    param_models = []\n",
    "    \n",
    "    train_loss_param = np.zeros((len(tuning), num_epochs))\n",
    "    train_acc_param = np.zeros((len(tuning), num_epochs))\n",
    "    val_loss_param = np.zeros((len(tuning), num_epochs))\n",
    "    val_acc_param = np.zeros((len(tuning), num_epochs))\n",
    "    \n",
    "    # hyperparameter tune\n",
    "    for idx, param in enumerate(tuning):\n",
    "        print(f'\\n HIDDEN_SIZE {param}')\n",
    "        print('--------------------------------')\n",
    "        \n",
    "        # define models to be analyzed\n",
    "        model_rnn = RNN(512, param, 6)\n",
    "        optimizer = optim.Adam(model_rnn.parameters(), lr = lr)\n",
    "\n",
    "        # train and validate model\n",
    "        train_loss, train_accuracies, valid_loss, valid_accuracies = train_nn(\n",
    "                                                                        model = model_rnn, \n",
    "                                                                        trainloader = trainloader, \n",
    "                                                                        valloader = valloader,\n",
    "                                                                        loss_function = loss_function,\n",
    "                                                                        optimizer = optimizer,\n",
    "                                                                        num_epochs = num_epochs,\n",
    "                                                                        val_step = val_step)\n",
    "        \n",
    "        # save models and losses\n",
    "        param_models.append(model_rnn)   \n",
    "        train_loss_param[idx] = train_loss\n",
    "        val_loss_param[idx] = valid_loss   \n",
    "    \n",
    "    # test for the best model\n",
    "    best_param_idx = val_loss_param[:, -1].argmin()\n",
    "    best_model = param_models[best_param_idx]\n",
    "    \n",
    "    print(f\"\\nbest params for fold {fold + 1}: \", tuning[best_param_idx])  \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        best_model.eval()\n",
    "        test_batch_loss = []\n",
    "        \n",
    "        for batch_idx, batch in enumerate(testloader):\n",
    "            inputs, labels, lengths = batch['data'], batch['labels'], batch['lengths']\n",
    "            \n",
    "            output = best_model(inputs)\n",
    "            \n",
    "            loss = 0\n",
    "            for l in range(output.shape[0]):\n",
    "                # masking the zero-padded outputs\n",
    "                batch_output = output[l][:lengths[l]]\n",
    "                batch_labels = labels[l][:lengths[l]]\n",
    "                \n",
    "                # compute cross-entropy loss\n",
    "                loss += loss_function(batch_output, batch_labels)\n",
    "                \n",
    "            # receive validation loss from current batch\n",
    "            test_batch_loss.append(loss.item())\n",
    "    \n",
    "    best_model.train()\n",
    "    \n",
    "    # save the best training loss\n",
    "    gen_train_loss[fold] = train_loss_param[best_param_idx]\n",
    "    fold_test_loss[fold] = np.mean(test_batch_loss)\n",
    "    \n",
    "    print(f\"test loss for fold {fold + 1}: \", fold_test_loss[fold], \"\\n\")\n",
    "\n",
    "# generalization loss\n",
    "gen_test_loss = np.mean(fold_test_loss)\n",
    "\n",
    "print(\"\\n generalization test loss: \", gen_test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_exam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
