{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import cycle, islice\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import json\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, pad_sequence\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from collections import defaultdict\n",
    "\n",
    "from classifier_rnn import RNN\n",
    "from accuracy import test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, folder_path, split):\n",
    "        self.folder_path = folder_path\n",
    "        self.split = split\n",
    "        self.data = self.load_data()\n",
    "        \n",
    "        self.label_encoding = {'I': 0, 'O': 1, 'P': 2, 'S': 3, 'M': 4, 'B': 5}\n",
    "        self.one_hot = LabelBinarizer()\n",
    "        self.one_hot.fit(list(self.label_encoding.values()))\n",
    "\n",
    "    def load_data(self):\n",
    "        file_list = [f for f in os.listdir(self.folder_path) if f.endswith('.npy')]\n",
    "        file_list.sort()  # Make sure the order is consistent\n",
    "\n",
    "        data = []\n",
    "        for file_name in file_list:\n",
    "            file_path = os.path.join(self.folder_path, file_name)\n",
    "            data.append(np.load(file_path, allow_pickle=True).item())\n",
    "\n",
    "        return data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sample = self.data[index]\n",
    "        #print(sample['data'])\n",
    "        inputs = sample['data']\n",
    "        labels_str = sample['labels']\n",
    "\n",
    "        labels_list = [self.label_encoding[label] for label in labels_str]\n",
    "        labels_list = self.one_hot.transform(labels_list)\n",
    "        \n",
    "        labels_tensor = torch.tensor(labels_list, dtype=torch.float)\n",
    "\n",
    "        return {'data': inputs, 'labels': labels_tensor}\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # sort the batch by sequence length in descending order\n",
    "    batch = sorted(batch, key=lambda x: len(x['data']), reverse=True)\n",
    "    \n",
    "    # pad sequences for data\n",
    "    data = [torch.tensor(sample['data']) for sample in batch]\n",
    "    padded_data = pad_sequence(data, batch_first=True)\n",
    "\n",
    "    # Pad sequences for labels\n",
    "    labels = [torch.tensor(sample['labels']) for sample in batch]\n",
    "    padded_labels = pad_sequence(labels, batch_first=True)\n",
    "    \n",
    "    # Pack the padded sequences for data\n",
    "    lengths = [len(seq) for seq in data]\n",
    "    #packed_data = pack_padded_sequence(padded_data, lengths=lengths, batch_first=True, enforce_sorted=True)\n",
    "\n",
    "    return {'data': padded_data, 'labels': padded_labels, \"lengths\": lengths} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nn(model, trainloader, valloader, loss_function, optimizer, fold, experiment_file_path, num_epochs = 50, val_step = 100):\n",
    "    # step = 0\n",
    "    model.train()\n",
    "    \n",
    "    train_loss = []\n",
    "    # train_accuracies = []\n",
    "    valid_loss = []\n",
    "    # valid_accuracies = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # train_batch_loss = []\n",
    "        # train_batch_accuracies = []\n",
    "        # val_batch_loss = []\n",
    "        # val_batch_accuracies = []\n",
    "        \n",
    "        for _, batch in enumerate(trainloader):\n",
    "            inputs, labels, lengths = batch['data'], batch['labels'], batch['lengths']\n",
    "\n",
    "            # receive output from rnn\n",
    "            output = model(inputs)  \n",
    "            \n",
    "            loss = 0\n",
    "            for l in range(output.shape[0]):\n",
    "                # masking the zero-padded outputs\n",
    "                batch_output = output[l][:lengths[l]]\n",
    "                batch_labels = labels[l][:lengths[l]]\n",
    "                \n",
    "                # compute cross-entropy loss\n",
    "                loss += loss_function(batch_output, batch_labels)\n",
    "                \n",
    "            # receive final loss from current batch    \n",
    "            # train_batch_loss.append(loss.item())\n",
    "            \n",
    "            # gradient update\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()  \n",
    "        \n",
    "        train_loss_epoch = test_predictions(model = model, loader = trainloader, loss_function = loss_function,\n",
    "                         cv = str(fold), experiment_file_path = experiment_file_path, condition = \"train\", epoch = str(epoch))   \n",
    "\n",
    "        valid_loss_epoch = test_predictions(model = model, loader = valloader, loss_function = loss_function,\n",
    "                        cv = str(fold), experiment_file_path = experiment_file_path, condition = \"val\", epoch = str(epoch))\n",
    "        \n",
    "        # # receive mean loss for this epoch\n",
    "        train_loss.append(train_loss_epoch)\n",
    "        valid_loss.append(valid_loss_epoch)\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            print(\"training loss: \", train_loss[-1], \\\n",
    "                \"\\t validation loss: \", valid_loss[-1])\n",
    "    \n",
    "    #return train_loss, train_accuracies, valid_loss, valid_accuracies\n",
    "    \n",
    "    return train_loss, valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD 1\n",
      "--------------------------------\n",
      "\n",
      "HIDDEN_SIZE 100\n",
      "--------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zs/b0y2x66j1kx9mlgj_2cpj7bw0000gp/T/ipykernel_29054/2825261995.py:47: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = [torch.tensor(sample['labels']) for sample in batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss:  15.953169822692871 \t validation loss:  5.319646835327148\n",
      "training loss:  15.830192565917969 \t validation loss:  5.272371768951416\n",
      "training loss:  15.690476417541504 \t validation loss:  5.2172136306762695\n",
      "training loss:  15.529463768005371 \t validation loss:  5.151750564575195\n",
      "training loss:  15.344619750976562 \t validation loss:  5.074643135070801\n",
      "training loss:  15.137310028076172 \t validation loss:  4.986494541168213\n",
      "training loss:  14.91318130493164 \t validation loss:  4.8901214599609375\n",
      "training loss:  14.679226875305176 \t validation loss:  4.790101051330566\n",
      "training loss:  14.440314292907715 \t validation loss:  4.690907955169678\n",
      "training loss:  14.198622703552246 \t validation loss:  4.595090866088867\n",
      "\n",
      "HIDDEN_SIZE 200\n",
      "--------------------------------\n",
      "training loss:  15.947739601135254 \t validation loss:  5.308323383331299\n",
      "training loss:  15.739425659179688 \t validation loss:  5.22324275970459\n",
      "training loss:  15.488553047180176 \t validation loss:  5.117068290710449\n",
      "training loss:  15.180575370788574 \t validation loss:  4.984732151031494\n",
      "training loss:  14.816865921020508 \t validation loss:  4.831043243408203\n",
      "training loss:  14.42492961883545 \t validation loss:  4.671009063720703\n",
      "training loss:  14.051251411437988 \t validation loss:  4.5202436447143555\n",
      "training loss:  13.74654769897461 \t validation loss:  4.390159606933594\n",
      "training loss:  13.529533386230469 \t validation loss:  4.28185510635376\n",
      "training loss:  13.328261375427246 \t validation loss:  4.186844825744629\n",
      "\n",
      "HIDDEN_SIZE 300\n",
      "--------------------------------\n",
      "training loss:  15.880539894104004 \t validation loss:  5.272773265838623\n",
      "training loss:  15.587366104125977 \t validation loss:  5.1485981941223145\n",
      "training loss:  15.202094078063965 \t validation loss:  4.980481147766113\n",
      "training loss:  14.702447891235352 \t validation loss:  4.761928558349609\n",
      "training loss:  14.13695240020752 \t validation loss:  4.515212059020996\n",
      "training loss:  13.626312255859375 \t validation loss:  4.276050567626953\n",
      "training loss:  13.33388900756836 \t validation loss:  4.095398902893066\n",
      "training loss:  12.852994918823242 \t validation loss:  4.098092555999756\n",
      "training loss:  13.023666381835938 \t validation loss:  4.656989574432373\n",
      "training loss:  12.256021499633789 \t validation loss:  3.952404260635376\n",
      "\n",
      "HIDDEN_SIZE 400\n",
      "--------------------------------\n",
      "training loss:  15.834582328796387 \t validation loss:  5.257008075714111\n",
      "training loss:  15.454487800598145 \t validation loss:  5.08950662612915\n",
      "training loss:  14.91904067993164 \t validation loss:  4.844844818115234\n",
      "training loss:  14.319205284118652 \t validation loss:  4.585294723510742\n",
      "training loss:  13.816563606262207 \t validation loss:  4.393126487731934\n",
      "training loss:  13.669576644897461 \t validation loss:  4.235713958740234\n",
      "training loss:  13.367286682128906 \t validation loss:  4.067304611206055\n",
      "training loss:  13.463922500610352 \t validation loss:  4.665746688842773\n",
      "training loss:  12.83257007598877 \t validation loss:  3.993760108947754\n",
      "training loss:  12.901863098144531 \t validation loss:  3.930849075317383\n",
      "\n",
      "best params for fold 1:  400\n",
      "test loss for fold 1:  4.332398891448975\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "\n",
      "HIDDEN_SIZE 100\n",
      "--------------------------------\n",
      "training loss:  16.046905517578125 \t validation loss:  5.350773811340332\n",
      "training loss:  15.927727699279785 \t validation loss:  5.3082275390625\n",
      "training loss:  15.796428680419922 \t validation loss:  5.261698246002197\n",
      "training loss:  15.647449493408203 \t validation loss:  5.209246635437012\n",
      "training loss:  15.475878715515137 \t validation loss:  5.149402618408203\n",
      "training loss:  15.278366088867188 \t validation loss:  5.081501007080078\n",
      "training loss:  15.054244995117188 \t validation loss:  5.006168842315674\n",
      "training loss:  14.806622505187988 \t validation loss:  4.92567253112793\n",
      "training loss:  14.543176651000977 \t validation loss:  4.843911170959473\n",
      "training loss:  14.275928497314453 \t validation loss:  4.7658233642578125\n",
      "\n",
      "HIDDEN_SIZE 200\n",
      "--------------------------------\n",
      "training loss:  15.959060668945312 \t validation loss:  5.3188395500183105\n",
      "training loss:  15.746173858642578 \t validation loss:  5.239972114562988\n",
      "training loss:  15.482168197631836 \t validation loss:  5.142234802246094\n",
      "training loss:  15.146984100341797 \t validation loss:  5.021880626678467\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/helenakeitum/Desktop/DeepLearningProtein/analysis.ipynb Cell 4\u001b[0m line \u001b[0;36m7\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/helenakeitum/Desktop/DeepLearningProtein/analysis.ipynb#X14sZmlsZQ%3D%3D?line=67'>68</a>\u001b[0m optimizer \u001b[39m=\u001b[39m optim\u001b[39m.\u001b[39mAdam(model_rnn\u001b[39m.\u001b[39mparameters(), lr \u001b[39m=\u001b[39m lr)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/helenakeitum/Desktop/DeepLearningProtein/analysis.ipynb#X14sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m \u001b[39m# train and validate model\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/helenakeitum/Desktop/DeepLearningProtein/analysis.ipynb#X14sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m train_loss, valid_loss \u001b[39m=\u001b[39m train_nn(model \u001b[39m=\u001b[39;49m model_rnn, \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/helenakeitum/Desktop/DeepLearningProtein/analysis.ipynb#X14sZmlsZQ%3D%3D?line=71'>72</a>\u001b[0m                                   trainloader \u001b[39m=\u001b[39;49m trainloader, valloader \u001b[39m=\u001b[39;49m valloader,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/helenakeitum/Desktop/DeepLearningProtein/analysis.ipynb#X14sZmlsZQ%3D%3D?line=72'>73</a>\u001b[0m                                   loss_function \u001b[39m=\u001b[39;49m loss_function, optimizer \u001b[39m=\u001b[39;49m optimizer, \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/helenakeitum/Desktop/DeepLearningProtein/analysis.ipynb#X14sZmlsZQ%3D%3D?line=73'>74</a>\u001b[0m                                   fold \u001b[39m=\u001b[39;49m fold, experiment_file_path \u001b[39m=\u001b[39;49m experiment_file_path, \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/helenakeitum/Desktop/DeepLearningProtein/analysis.ipynb#X14sZmlsZQ%3D%3D?line=74'>75</a>\u001b[0m                                   num_epochs \u001b[39m=\u001b[39;49m num_epochs, val_step \u001b[39m=\u001b[39;49m val_step)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/helenakeitum/Desktop/DeepLearningProtein/analysis.ipynb#X14sZmlsZQ%3D%3D?line=76'>77</a>\u001b[0m \u001b[39m# train_loss, train_accuracies, valid_loss, valid_accuracies = train_nn(\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/helenakeitum/Desktop/DeepLearningProtein/analysis.ipynb#X14sZmlsZQ%3D%3D?line=77'>78</a>\u001b[0m \u001b[39m#                                                                 model = model_rnn, \u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/helenakeitum/Desktop/DeepLearningProtein/analysis.ipynb#X14sZmlsZQ%3D%3D?line=78'>79</a>\u001b[0m \u001b[39m#                                                                 trainloader = trainloader, \u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/helenakeitum/Desktop/DeepLearningProtein/analysis.ipynb#X14sZmlsZQ%3D%3D?line=86'>87</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/helenakeitum/Desktop/DeepLearningProtein/analysis.ipynb#X14sZmlsZQ%3D%3D?line=87'>88</a>\u001b[0m \u001b[39m# save models and losses\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/helenakeitum/Desktop/DeepLearningProtein/analysis.ipynb#X14sZmlsZQ%3D%3D?line=88'>89</a>\u001b[0m param_models\u001b[39m.\u001b[39mappend(model_rnn)   \n",
      "\u001b[1;32m/Users/helenakeitum/Desktop/DeepLearningProtein/analysis.ipynb Cell 4\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/helenakeitum/Desktop/DeepLearningProtein/analysis.ipynb#X14sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m     \u001b[39m# receive final loss from current batch    \u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/helenakeitum/Desktop/DeepLearningProtein/analysis.ipynb#X14sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m     \u001b[39m# train_batch_loss.append(loss.item())\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/helenakeitum/Desktop/DeepLearningProtein/analysis.ipynb#X14sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m     \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/helenakeitum/Desktop/DeepLearningProtein/analysis.ipynb#X14sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m     \u001b[39m# gradient update\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/helenakeitum/Desktop/DeepLearningProtein/analysis.ipynb#X14sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/helenakeitum/Desktop/DeepLearningProtein/analysis.ipynb#X14sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m     loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/helenakeitum/Desktop/DeepLearningProtein/analysis.ipynb#X14sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mstep()  \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/helenakeitum/Desktop/DeepLearningProtein/analysis.ipynb#X14sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m train_loss_epoch \u001b[39m=\u001b[39m test_predictions(model \u001b[39m=\u001b[39m model, loader \u001b[39m=\u001b[39m trainloader, loss_function \u001b[39m=\u001b[39m loss_function,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/helenakeitum/Desktop/DeepLearningProtein/analysis.ipynb#X14sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m                  cv \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(fold), experiment_file_path \u001b[39m=\u001b[39m experiment_file_path, condition \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m, epoch \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(epoch))   \n",
      "File \u001b[0;32m~/miniconda3/envs/dl_exam/lib/python3.9/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    493\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    494\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/dl_exam/lib/python3.9/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m     tensors,\n\u001b[1;32m    253\u001b[0m     grad_tensors_,\n\u001b[1;32m    254\u001b[0m     retain_graph,\n\u001b[1;32m    255\u001b[0m     create_graph,\n\u001b[1;32m    256\u001b[0m     inputs,\n\u001b[1;32m    257\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    258\u001b[0m     accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    259\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#ensure that all tensors are on the GPU\n",
    "if torch.cuda.is_available():\n",
    "    torch.set_default_device(\"cuda:0\")\n",
    "\n",
    "# config\n",
    "k_folds = 5\n",
    "num_epochs = 5\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "lr = 1e-3\n",
    "tuning = [100, 200, 300, 400]\n",
    "val_step = 32\n",
    "\n",
    "experiment_file_list = []\n",
    "for i in tuning:\n",
    "    experiment_file_list.append(f\"stat_data_{i}\")\n",
    "    experiment_json = {}\n",
    "    open(experiment_file_list[-1], 'w').write(json.dumps(experiment_json))\n",
    "\n",
    "# set fixed random number seed\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# create cv's corresponding to deeptmhmm's\n",
    "cvs = list(range(5))\n",
    "kfolds = []\n",
    "for idx, split in enumerate(range(5)):\n",
    "    \n",
    "    # make cycling list and define train/val/test splits\n",
    "    idxs = np.asarray(list(islice(cycle(cvs), idx, idx + 5)))\n",
    "    train_idx, val_idx, test_idx = idxs[:3], idxs[3], idxs[4]\n",
    "    \n",
    "    kfolds.append((train_idx, val_idx, test_idx))\n",
    "\n",
    "# make on big concatenated dataset of all splits\n",
    "data_cvs = np.squeeze([CustomDataset(os.path.join(\"encoder_proteins_test\", folder), 'train') for folder in ['cv0', 'cv1', 'cv2', 'cv3' , 'cv4']])\n",
    "\n",
    "gen_train_loss = np.zeros((len(kfolds), num_epochs))\n",
    "gen_train_acc = np.zeros((len(kfolds), num_epochs))\n",
    "fold_test_loss = np.zeros((len(kfolds)))\n",
    "fold_test_acc = np.zeros((len(kfolds)))\n",
    "\n",
    "# k-fold cross validation\n",
    "for fold, (train_ids, val_id, test_id) in enumerate(kfolds):    \n",
    "    print(f'FOLD {fold + 1}')\n",
    "    print('--------------------------------')\n",
    "    \n",
    "    # concatenates the data from the different cv's\n",
    "    training_data = np.concatenate(data_cvs[train_ids], axis = 0)\n",
    "    \n",
    "    # define data loaders for train/val/test data in this fold (collate 0 pads for same-length)\n",
    "    trainloader = DataLoader(\n",
    "                        training_data, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "    valloader = DataLoader(data_cvs[val_id], batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "    testloader = DataLoader(data_cvs[test_id], batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "    \n",
    "    param_models = []\n",
    "    \n",
    "    train_loss_param = np.zeros((len(tuning), num_epochs))\n",
    "    train_acc_param = np.zeros((len(tuning), num_epochs))\n",
    "    val_loss_param = np.zeros((len(tuning), num_epochs))\n",
    "    val_acc_param = np.zeros((len(tuning), num_epochs))\n",
    "    \n",
    "    # hyperparameter tune\n",
    "    for idx, param in enumerate(tuning):\n",
    "        experiment_file_path = experiment_file_list[idx]\n",
    "        \n",
    "        print(f'\\nHIDDEN_SIZE {param}')\n",
    "        print('--------------------------------')\n",
    "        \n",
    "        # define models to be analyzed\n",
    "        model_rnn = RNN(512, param, 6)\n",
    "        optimizer = optim.Adam(model_rnn.parameters(), lr = lr)\n",
    "\n",
    "        # train and validate model\n",
    "        train_loss, valid_loss = train_nn(model = model_rnn, \n",
    "                                          trainloader = trainloader, valloader = valloader,\n",
    "                                          loss_function = loss_function, optimizer = optimizer, \n",
    "                                          fold = fold, experiment_file_path = experiment_file_path, \n",
    "                                          num_epochs = num_epochs, val_step = val_step)\n",
    "        \n",
    "        # train_loss, train_accuracies, valid_loss, valid_accuracies = train_nn(\n",
    "        #                                                                 model = model_rnn, \n",
    "        #                                                                 trainloader = trainloader, \n",
    "        #                                                                 valloader = valloader,\n",
    "        #                                                                 loss_function = loss_function,\n",
    "        #                                                                 optimizer = optimizer,\n",
    "        #                                                                 fold = fold,\n",
    "        #                                                                 experiment_file_path = experiment_file_path,\n",
    "        #                                                                 num_epochs = num_epochs,\n",
    "        #                                                                 val_step = val_step)\n",
    "        \n",
    "        # save models and losses\n",
    "        param_models.append(model_rnn)   \n",
    "        train_loss_param[idx] = train_loss\n",
    "        val_loss_param[idx] = valid_loss   \n",
    "    \n",
    "    # test for the best model\n",
    "    best_param_idx = val_loss_param[:, -1].argmin()\n",
    "    best_model = param_models[best_param_idx]\n",
    "    experiment_file_path = experiment_file_list[best_param_idx]\n",
    "    \n",
    "    print(f\"\\nbest params for fold {fold + 1}: \", tuning[best_param_idx])  \n",
    "    \n",
    "    test_loss = test_predictions(model = best_model, \n",
    "                    loader = testloader, \n",
    "                    loss_function = loss_function, \n",
    "                    cv = str(fold), experiment_file_path = experiment_file_path)\n",
    "    \n",
    "    # with torch.no_grad():\n",
    "    #     best_model.eval()\n",
    "    #     test_batch_loss = []\n",
    "        \n",
    "    #     for batch_idx, batch in enumerate(testloader):\n",
    "    #         inputs, labels, lengths = batch['data'], batch['labels'], batch['lengths']\n",
    "            \n",
    "    #         output = best_model(inputs)\n",
    "            \n",
    "    #         loss = 0\n",
    "    #         for l in range(output.shape[0]):\n",
    "    #             # masking the zero-padded outputs\n",
    "    #             batch_output = output[l][:lengths[l]]\n",
    "    #             batch_labels = labels[l][:lengths[l]]\n",
    "                \n",
    "    #             # compute cross-entropy loss\n",
    "    #             loss += loss_function(batch_output, batch_labels)\n",
    "                \n",
    "    #         # receive validation loss from current batch\n",
    "    #         test_batch_loss.append(loss.item())\n",
    "    \n",
    "    # best_model.train()\n",
    "    \n",
    "    # save the best training loss\n",
    "    # gen_train_loss[fold] = train_loss_param[best_param_idx]\n",
    "    # fold_test_loss[fold] = np.mean(test_batch_loss)\n",
    "    \n",
    "    print(f\"test loss for fold {fold + 1}: \", test_loss)\n",
    "\n",
    "# generalization loss\n",
    "# gen_test_loss = np.mean(fold_test_loss)\n",
    "\n",
    "# print(\"\\n generalization test loss: \", gen_test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_exam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
