{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import cycle, islice\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import json\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, pad_sequence\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from collections import defaultdict\n",
    "\n",
    "from classifier_rnn import RNN\n",
    "from accuracy import test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, folder_path, split):\n",
    "        self.folder_path = folder_path\n",
    "        self.split = split\n",
    "        self.data = self.load_data()\n",
    "        \n",
    "        self.label_encoding = {'I': 0, 'O': 1, 'P': 2, 'S': 3, 'M': 4, 'B': 5}\n",
    "        self.one_hot = LabelBinarizer()\n",
    "        self.one_hot.fit(list(self.label_encoding.values()))\n",
    "\n",
    "    def load_data(self):\n",
    "        file_list = [f for f in os.listdir(self.folder_path) if f.endswith('.npy')]\n",
    "        file_list.sort()  # Make sure the order is consistent\n",
    "\n",
    "        data = []\n",
    "        for file_name in file_list:\n",
    "            file_path = os.path.join(self.folder_path, file_name)\n",
    "            data.append(np.load(file_path, allow_pickle=True).item())\n",
    "\n",
    "        return data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sample = self.data[index]\n",
    "        #print(sample['data'])\n",
    "        inputs = sample['data']\n",
    "        labels_str = sample['labels']\n",
    "\n",
    "        labels_list = [self.label_encoding[label] for label in labels_str]\n",
    "        labels_list = self.one_hot.transform(labels_list)\n",
    "        \n",
    "        labels_tensor = torch.tensor(labels_list, dtype=torch.float)\n",
    "\n",
    "        return {'data': inputs, 'labels': labels_tensor}\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # sort the batch by sequence length in descending order\n",
    "    batch = sorted(batch, key=lambda x: len(x['data']), reverse=True)\n",
    "    \n",
    "    # pad sequences for data\n",
    "    data = [torch.tensor(sample['data']) for sample in batch]\n",
    "    padded_data = pad_sequence(data, batch_first=True)\n",
    "\n",
    "    # Pad sequences for labels\n",
    "    labels = [torch.tensor(sample['labels']) for sample in batch]\n",
    "    padded_labels = pad_sequence(labels, batch_first=True)\n",
    "    \n",
    "    # Pack the padded sequences for data\n",
    "    lengths = [len(seq) for seq in data]\n",
    "    #packed_data = pack_padded_sequence(padded_data, lengths=lengths, batch_first=True, enforce_sorted=True)\n",
    "\n",
    "    return {'data': padded_data, 'labels': padded_labels, \"lengths\": lengths} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nn(model, trainloader, valloader, loss_function, optimizer, num_epochs = 50, val_step = 100):\n",
    "    step = 0\n",
    "    model.train()\n",
    "    \n",
    "    train_loss = []\n",
    "    train_accuracies = []\n",
    "    valid_loss = []\n",
    "    valid_accuracies = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        train_batch_loss = []\n",
    "        train_batch_accuracies = []\n",
    "        val_batch_loss = []\n",
    "        val_batch_accuracies = []\n",
    "        \n",
    "        for _, batch in enumerate(trainloader):\n",
    "            inputs, labels, lengths = batch['data'], batch['labels'], batch['lengths']\n",
    "\n",
    "            # receive output from rnn\n",
    "            output = model(inputs)  \n",
    "\n",
    "            loss = 0\n",
    "            for l in range(output.shape[0]):\n",
    "                # masking the zero-padded outputs\n",
    "                batch_output = output[l][:lengths[l]]\n",
    "                batch_labels = labels[l][:lengths[l]]\n",
    "                \n",
    "                # compute cross-entropy loss\n",
    "                loss += loss_function(batch_output, batch_labels)\n",
    "            \n",
    "            # receive final loss from current batch    \n",
    "            train_batch_loss.append(loss.item())\n",
    "            \n",
    "            # gradient update\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()  \n",
    "            \n",
    "            step += 1\n",
    "            \n",
    "            # consider only doing this once done with training (after epoch loop)\n",
    "            if step % val_step == 0:\n",
    "                # validate current model with hidden_size = param\n",
    "                with torch.no_grad():\n",
    "                    model.eval()\n",
    "                    \n",
    "                    for _, batch in enumerate(valloader):\n",
    "                        inputs, labels, lengths = batch['data'], batch['labels'], batch['lengths']\n",
    "                        output = model(inputs)\n",
    "                        \n",
    "                        loss = 0\n",
    "                        for l in range(output.shape[0]):\n",
    "                            # masking the zero-padded outputs\n",
    "                            batch_output = output[l][:lengths[l]]\n",
    "                            batch_labels = labels[l][:lengths[l]]\n",
    "                            \n",
    "                            # compute cross-entropy loss\n",
    "                            loss += loss_function(batch_output, batch_labels)\n",
    "                        \n",
    "                        # receive validation loss from current batch\n",
    "                        val_batch_loss.append(loss.item())\n",
    "                        \n",
    "                    model.train()\n",
    "                \n",
    "        # receive mean loss for this epoch\n",
    "        train_loss.append(np.mean(train_batch_loss))\n",
    "        valid_loss.append(np.mean(val_batch_loss))\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            print(\"training loss: \", train_loss[-1], \\\n",
    "                \"\\t validation loss: \", valid_loss[-1])\n",
    "\n",
    "    return train_loss, train_accuracies, valid_loss, valid_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD 1\n",
      "--------------------------------\n",
      "\n",
      "HIDDEN_SIZE 100\n",
      "--------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zs/b0y2x66j1kx9mlgj_2cpj7bw0000gp/T/ipykernel_21333/2825261995.py:47: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = [torch.tensor(sample['labels']) for sample in batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss:  16.064556121826172 \t validation loss:  5.319646835327148\n",
      "\n",
      "HIDDEN_SIZE 200\n",
      "--------------------------------\n",
      "training loss:  16.12388038635254 \t validation loss:  5.293364524841309\n",
      "\n",
      "HIDDEN_SIZE 300\n",
      "--------------------------------\n",
      "training loss:  16.12946128845215 \t validation loss:  5.272335529327393\n",
      "\n",
      "HIDDEN_SIZE 400\n",
      "--------------------------------\n",
      "training loss:  16.076438903808594 \t validation loss:  5.234174728393555\n",
      "\n",
      "best params for fold 1:  400\n",
      "test loss for fold 1:  4.4634504318237305 \n",
      " test loss from accuracy.py.  4.4634504318237305\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "\n",
      "HIDDEN_SIZE 100\n",
      "--------------------------------\n",
      "training loss:  16.099788665771484 \t validation loss:  5.340958595275879\n",
      "\n",
      "HIDDEN_SIZE 200\n",
      "--------------------------------\n",
      "training loss:  16.162357330322266 \t validation loss:  5.309473991394043\n",
      "\n",
      "HIDDEN_SIZE 300\n",
      "--------------------------------\n",
      "training loss:  16.143686294555664 \t validation loss:  5.29081916809082\n",
      "\n",
      "HIDDEN_SIZE 400\n",
      "--------------------------------\n",
      "training loss:  16.14583969116211 \t validation loss:  5.259993553161621\n",
      "\n",
      "best params for fold 2:  400\n",
      "test loss for fold 2:  4.65471887588501 \n",
      " test loss from accuracy.py.  4.65471887588501\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "\n",
      "HIDDEN_SIZE 100\n",
      "--------------------------------\n",
      "training loss:  16.113773345947266 \t validation loss:  5.338517189025879\n",
      "\n",
      "HIDDEN_SIZE 200\n",
      "--------------------------------\n",
      "training loss:  16.156173706054688 \t validation loss:  5.3418989181518555\n",
      "\n",
      "HIDDEN_SIZE 300\n",
      "--------------------------------\n",
      "training loss:  16.11141014099121 \t validation loss:  5.31102180480957\n",
      "\n",
      "HIDDEN_SIZE 400\n",
      "--------------------------------\n",
      "training loss:  16.097457885742188 \t validation loss:  5.290604591369629\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/helenakeitum/Desktop/DeepLearningProtein/analysis.ipynb Cell 4\u001b[0m line \u001b[0;36m6\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/helenakeitum/Desktop/DeepLearningProtein/analysis.ipynb#X14sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m optimizer \u001b[39m=\u001b[39m optim\u001b[39m.\u001b[39mAdam(model_rnn\u001b[39m.\u001b[39mparameters(), lr \u001b[39m=\u001b[39m lr)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/helenakeitum/Desktop/DeepLearningProtein/analysis.ipynb#X14sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m \u001b[39m# train and validate model\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/helenakeitum/Desktop/DeepLearningProtein/analysis.ipynb#X14sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m train_loss, train_accuracies, valid_loss, valid_accuracies \u001b[39m=\u001b[39m train_nn(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/helenakeitum/Desktop/DeepLearningProtein/analysis.ipynb#X14sZmlsZQ%3D%3D?line=67'>68</a>\u001b[0m                                                                 model \u001b[39m=\u001b[39;49m model_rnn, \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/helenakeitum/Desktop/DeepLearningProtein/analysis.ipynb#X14sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m                                                                 trainloader \u001b[39m=\u001b[39;49m trainloader, \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/helenakeitum/Desktop/DeepLearningProtein/analysis.ipynb#X14sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m                                                                 valloader \u001b[39m=\u001b[39;49m valloader,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/helenakeitum/Desktop/DeepLearningProtein/analysis.ipynb#X14sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m                                                                 loss_function \u001b[39m=\u001b[39;49m loss_function,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/helenakeitum/Desktop/DeepLearningProtein/analysis.ipynb#X14sZmlsZQ%3D%3D?line=71'>72</a>\u001b[0m                                                                 optimizer \u001b[39m=\u001b[39;49m optimizer,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/helenakeitum/Desktop/DeepLearningProtein/analysis.ipynb#X14sZmlsZQ%3D%3D?line=72'>73</a>\u001b[0m                                                                 num_epochs \u001b[39m=\u001b[39;49m num_epochs,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/helenakeitum/Desktop/DeepLearningProtein/analysis.ipynb#X14sZmlsZQ%3D%3D?line=73'>74</a>\u001b[0m                                                                 val_step \u001b[39m=\u001b[39;49m val_step)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/helenakeitum/Desktop/DeepLearningProtein/analysis.ipynb#X14sZmlsZQ%3D%3D?line=75'>76</a>\u001b[0m \u001b[39m# save models and losses\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/helenakeitum/Desktop/DeepLearningProtein/analysis.ipynb#X14sZmlsZQ%3D%3D?line=76'>77</a>\u001b[0m param_models\u001b[39m.\u001b[39mappend(model_rnn)   \n",
      "\u001b[1;32m/Users/helenakeitum/Desktop/DeepLearningProtein/analysis.ipynb Cell 4\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/helenakeitum/Desktop/DeepLearningProtein/analysis.ipynb#X14sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m \u001b[39mfor\u001b[39;00m _, batch \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(valloader):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/helenakeitum/Desktop/DeepLearningProtein/analysis.ipynb#X14sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m     inputs, labels, lengths \u001b[39m=\u001b[39m batch[\u001b[39m'\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m'\u001b[39m], batch[\u001b[39m'\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m'\u001b[39m], batch[\u001b[39m'\u001b[39m\u001b[39mlengths\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/helenakeitum/Desktop/DeepLearningProtein/analysis.ipynb#X14sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m     output \u001b[39m=\u001b[39m model(inputs)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/helenakeitum/Desktop/DeepLearningProtein/analysis.ipynb#X14sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/helenakeitum/Desktop/DeepLearningProtein/analysis.ipynb#X14sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m     \u001b[39mfor\u001b[39;00m l \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(output\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/helenakeitum/Desktop/DeepLearningProtein/analysis.ipynb#X14sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m         \u001b[39m# masking the zero-padded outputs\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/dl_exam/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/dl_exam/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/DeepLearningProtein/classifier_rnn.py:18\u001b[0m, in \u001b[0;36mRNN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m     17\u001b[0m     \u001b[39m# return output and last hidden state\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m     x, (h, c) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlstm(x)\n\u001b[1;32m     20\u001b[0m     \u001b[39m# fully-connected output layer\u001b[39;00m\n\u001b[1;32m     21\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msoftmax(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlinear(x))\n",
      "File \u001b[0;32m~/miniconda3/envs/dl_exam/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/dl_exam/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/dl_exam/lib/python3.9/site-packages/torch/nn/modules/rnn.py:879\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    876\u001b[0m         hx \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[1;32m    878\u001b[0m \u001b[39mif\u001b[39;00m batch_sizes \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 879\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39;49mlstm(\u001b[39minput\u001b[39;49m, hx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_flat_weights, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_layers,\n\u001b[1;32m    880\u001b[0m                       \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbidirectional, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_first)\n\u001b[1;32m    881\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    882\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39mlstm(\u001b[39minput\u001b[39m, batch_sizes, hx, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flat_weights, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias,\n\u001b[1;32m    883\u001b[0m                       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_layers, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbidirectional)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# config\n",
    "k_folds = 5\n",
    "num_epochs = 5\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "lr = 1e-3\n",
    "tuning = [100, 200, 300, 400]\n",
    "val_step = 1\n",
    "\n",
    "experiment_file_path = \"stat_data\"\n",
    "experiment_json = {}\n",
    "open(experiment_file_path, 'w').write(json.dumps(experiment_json))\n",
    "\n",
    "# set fixed random number seed\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# create cv's corresponding to deeptmhmm's\n",
    "cvs = list(range(5))\n",
    "kfolds = []\n",
    "for idx, split in enumerate(range(5)):\n",
    "    \n",
    "    # make cycling list and define train/val/test splits\n",
    "    idxs = np.asarray(list(islice(cycle(cvs), idx, idx + 5)))\n",
    "    train_idx, val_idx, test_idx = idxs[:3], idxs[3], idxs[4]\n",
    "    \n",
    "    kfolds.append((train_idx, val_idx, test_idx))\n",
    "\n",
    "# make on big concatenated dataset of all splits\n",
    "data_cvs = np.squeeze([CustomDataset(os.path.join(\"encoder_proteins_test\", folder), 'train') for folder in ['cv0', 'cv1', 'cv2', 'cv3' , 'cv4']])\n",
    "\n",
    "gen_train_loss = np.zeros((len(kfolds), num_epochs))\n",
    "gen_train_acc = np.zeros((len(kfolds), num_epochs))\n",
    "fold_test_loss = np.zeros((len(kfolds)))\n",
    "fold_test_acc = np.zeros((len(kfolds)))\n",
    "\n",
    "# k-fold cross validation\n",
    "for fold, (train_ids, val_id, test_id) in enumerate(kfolds):    \n",
    "    print(f'FOLD {fold + 1}')\n",
    "    print('--------------------------------')\n",
    "    \n",
    "    # concatenates the data from the different cv's\n",
    "    training_data = np.concatenate(data_cvs[train_ids], axis = 0)\n",
    "    \n",
    "    # define data loaders for train/val/test data in this fold (collate 0 pads for same-length)\n",
    "    trainloader = DataLoader(\n",
    "                        training_data, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "    valloader = DataLoader(data_cvs[val_id], batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "    testloader = DataLoader(data_cvs[test_id], batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "    \n",
    "    param_models = []\n",
    "    \n",
    "    train_loss_param = np.zeros((len(tuning), num_epochs))\n",
    "    train_acc_param = np.zeros((len(tuning), num_epochs))\n",
    "    val_loss_param = np.zeros((len(tuning), num_epochs))\n",
    "    val_acc_param = np.zeros((len(tuning), num_epochs))\n",
    "    \n",
    "    # hyperparameter tune\n",
    "    for idx, param in enumerate(tuning):\n",
    "        print(f'\\nHIDDEN_SIZE {param}')\n",
    "        print('--------------------------------')\n",
    "        \n",
    "        # define models to be analyzed\n",
    "        model_rnn = RNN(512, param, 6)\n",
    "        optimizer = optim.Adam(model_rnn.parameters(), lr = lr)\n",
    "\n",
    "        # train and validate model\n",
    "        train_loss, train_accuracies, valid_loss, valid_accuracies = train_nn(\n",
    "                                                                        model = model_rnn, \n",
    "                                                                        trainloader = trainloader, \n",
    "                                                                        valloader = valloader,\n",
    "                                                                        loss_function = loss_function,\n",
    "                                                                        optimizer = optimizer,\n",
    "                                                                        num_epochs = num_epochs,\n",
    "                                                                        val_step = val_step)\n",
    "        \n",
    "        # save models and losses\n",
    "        param_models.append(model_rnn)   \n",
    "        train_loss_param[idx] = train_loss\n",
    "        val_loss_param[idx] = valid_loss   \n",
    "    \n",
    "    # test for the best model\n",
    "    best_param_idx = val_loss_param[:, -1].argmin()\n",
    "    best_model = param_models[best_param_idx]\n",
    "    \n",
    "    print(f\"\\nbest params for fold {fold + 1}: \", tuning[best_param_idx])  \n",
    "    \n",
    "    _, _, _, loss_test = test_predictions(model = best_model, testloader = testloader, loss_function = loss_function, cv = fold, experiment_file_path = experiment_file_path)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        best_model.eval()\n",
    "        test_batch_loss = []\n",
    "        \n",
    "        for batch_idx, batch in enumerate(testloader):\n",
    "            inputs, labels, lengths = batch['data'], batch['labels'], batch['lengths']\n",
    "            \n",
    "            output = best_model(inputs)\n",
    "            \n",
    "            loss = 0\n",
    "            for l in range(output.shape[0]):\n",
    "                # masking the zero-padded outputs\n",
    "                batch_output = output[l][:lengths[l]]\n",
    "                batch_labels = labels[l][:lengths[l]]\n",
    "                \n",
    "                # compute cross-entropy loss\n",
    "                loss += loss_function(batch_output, batch_labels)\n",
    "                \n",
    "            # receive validation loss from current batch\n",
    "            test_batch_loss.append(loss.item())\n",
    "    \n",
    "    best_model.train()\n",
    "    \n",
    "    # save the best training loss\n",
    "    gen_train_loss[fold] = train_loss_param[best_param_idx]\n",
    "    fold_test_loss[fold] = np.mean(test_batch_loss)\n",
    "    \n",
    "    print(f\"test loss for fold {fold + 1}: \", fold_test_loss[fold], \"\\n\", \\\n",
    "        \"test loss from accuracy.py. \", loss_test)\n",
    "\n",
    "# generalization loss\n",
    "gen_test_loss = np.mean(fold_test_loss)\n",
    "\n",
    "print(\"\\n generalization test loss: \", gen_test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_exam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
