{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load modules\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import esm\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/helenakeitum/miniconda3/envs/inverse/lib/python3.9/site-packages/esm/pretrained.py:215: UserWarning: Regression weights not found, predicting contacts will not produce correct results.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# import model\n",
    "model, alphabet = esm.pretrained.esm_if1_gvp4_t16_142M_UR50()\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import labels\n",
    "f = open(\"DeepTMHMM.partitions.json\")\n",
    "labels = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved data for:  P39517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved data for:  P01161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved data for:  P0A407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved data for:  Q8R316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved data for:  Q15327\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved data for:  Q8TAX9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved data for:  Q60214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved data for:  Q9H8H2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved data for:  G3XD89\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved data for:  O60136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved data for:  Q9JLA3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved data for:  Q9QUS4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved data for:  Q8R570\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 238/717 [12:39<25:28,  3.19s/it]  \n",
      "  0%|          | 0/5 [12:39<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/helenakeitum/Desktop/DeepLearningProtein/create_latent_dataset.ipynb Cell 4\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/helenakeitum/Desktop/DeepLearningProtein/create_latent_dataset.ipynb#W3sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m coords, native_seq \u001b[39m=\u001b[39m esm\u001b[39m.\u001b[39minverse_folding\u001b[39m.\u001b[39mutil\u001b[39m.\u001b[39mextract_coords_from_structure(structure)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/helenakeitum/Desktop/DeepLearningProtein/create_latent_dataset.ipynb#W3sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39m# get encoder output as structure representation shape: (amino acid, encoder dimension)\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/helenakeitum/Desktop/DeepLearningProtein/create_latent_dataset.ipynb#W3sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m rep \u001b[39m=\u001b[39m esm\u001b[39m.\u001b[39;49minverse_folding\u001b[39m.\u001b[39;49mutil\u001b[39m.\u001b[39;49mget_encoder_output(model, alphabet, coords)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/helenakeitum/Desktop/DeepLearningProtein/create_latent_dataset.ipynb#W3sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m data[\u001b[39m\"\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m rep\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/helenakeitum/Desktop/DeepLearningProtein/create_latent_dataset.ipynb#W3sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m data[\u001b[39m\"\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m protein[\u001b[39m\"\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/inverse/lib/python3.9/site-packages/esm/inverse_folding/util.py:140\u001b[0m, in \u001b[0;36mget_encoder_output\u001b[0;34m(model, alphabet, coords)\u001b[0m\n\u001b[1;32m    137\u001b[0m batch \u001b[39m=\u001b[39m [(coords, \u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m)]\n\u001b[1;32m    138\u001b[0m coords, confidence, strs, tokens, padding_mask \u001b[39m=\u001b[39m batch_converter(\n\u001b[1;32m    139\u001b[0m     batch, device\u001b[39m=\u001b[39mdevice)\n\u001b[0;32m--> 140\u001b[0m encoder_out \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mencoder\u001b[39m.\u001b[39;49mforward(coords, padding_mask, confidence,\n\u001b[1;32m    141\u001b[0m         return_all_hiddens\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    142\u001b[0m \u001b[39m# remove beginning and end (bos and eos tokens)\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[39mreturn\u001b[39;00m encoder_out[\u001b[39m'\u001b[39m\u001b[39mencoder_out\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m0\u001b[39m][\u001b[39m1\u001b[39m:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/inverse/lib/python3.9/site-packages/esm/inverse_folding/gvp_transformer_encoder.py:154\u001b[0m, in \u001b[0;36mGVPTransformerEncoder.forward\u001b[0;34m(self, coords, encoder_padding_mask, confidence, return_all_hiddens)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    124\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    125\u001b[0m     coords,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    128\u001b[0m     return_all_hiddens: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    129\u001b[0m ):\n\u001b[1;32m    130\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[39m        coords (Tensor): backbone coordinates\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[39m              Only populated if *return_all_hiddens* is True.\u001b[39;00m\n\u001b[1;32m    153\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 154\u001b[0m     x, encoder_embedding \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward_embedding(coords,\n\u001b[1;32m    155\u001b[0m             encoder_padding_mask, confidence)\n\u001b[1;32m    156\u001b[0m     \u001b[39m# account for padding while computing the representation\u001b[39;00m\n\u001b[1;32m    157\u001b[0m     x \u001b[39m=\u001b[39m x \u001b[39m*\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m encoder_padding_mask\u001b[39m.\u001b[39munsqueeze(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mtype_as(x))\n",
      "File \u001b[0;32m~/miniconda3/envs/inverse/lib/python3.9/site-packages/esm/inverse_folding/gvp_transformer_encoder.py:91\u001b[0m, in \u001b[0;36mGVPTransformerEncoder.forward_embedding\u001b[0;34m(self, coords, padding_mask, confidence)\u001b[0m\n\u001b[1;32m     88\u001b[0m components[\u001b[39m\"\u001b[39m\u001b[39mdiherals\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_dihedrals(coords)\n\u001b[1;32m     90\u001b[0m \u001b[39m# GVP encoder\u001b[39;00m\n\u001b[0;32m---> 91\u001b[0m gvp_out_scalars, gvp_out_vectors \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgvp_encoder(coords,\n\u001b[1;32m     92\u001b[0m         coord_mask, padding_mask, confidence)\n\u001b[1;32m     93\u001b[0m R \u001b[39m=\u001b[39m get_rotation_frames(coords)\n\u001b[1;32m     94\u001b[0m \u001b[39m# Rotate to local rotation frame for rotation-invariance\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/inverse/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/inverse/lib/python3.9/site-packages/esm/inverse_folding/gvp_encoder.py:52\u001b[0m, in \u001b[0;36mGVPEncoder.forward\u001b[0;34m(self, coords, coord_mask, padding_mask, confidence)\u001b[0m\n\u001b[1;32m     48\u001b[0m node_embeddings, edge_embeddings, edge_index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_graph(\n\u001b[1;32m     49\u001b[0m         coords, coord_mask, padding_mask, confidence)\n\u001b[1;32m     51\u001b[0m \u001b[39mfor\u001b[39;00m i, layer \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder_layers):\n\u001b[0;32m---> 52\u001b[0m     node_embeddings, edge_embeddings \u001b[39m=\u001b[39m layer(node_embeddings,\n\u001b[1;32m     53\u001b[0m             edge_index, edge_embeddings)\n\u001b[1;32m     55\u001b[0m node_embeddings \u001b[39m=\u001b[39m unflatten_graph(node_embeddings, coords\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m])\n\u001b[1;32m     56\u001b[0m \u001b[39mreturn\u001b[39;00m node_embeddings\n",
      "File \u001b[0;32m~/miniconda3/envs/inverse/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/inverse/lib/python3.9/site-packages/esm/inverse_folding/gvp_modules.py:460\u001b[0m, in \u001b[0;36mGVPConvLayer.forward\u001b[0;34m(self, x, edge_index, edge_attr, autoregressive_x, node_mask)\u001b[0m\n\u001b[1;32m    457\u001b[0m     dh \u001b[39m=\u001b[39m dh[\u001b[39m0\u001b[39m] \u001b[39m/\u001b[39m count, dh[\u001b[39m1\u001b[39m] \u001b[39m/\u001b[39m count\u001b[39m.\u001b[39munsqueeze(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    459\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 460\u001b[0m     dh \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv(x, edge_index, edge_attr)\n\u001b[1;32m    462\u001b[0m \u001b[39mif\u001b[39;00m node_mask \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    463\u001b[0m     x_ \u001b[39m=\u001b[39m x\n",
      "File \u001b[0;32m~/miniconda3/envs/inverse/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/inverse/lib/python3.9/site-packages/esm/inverse_folding/gvp_modules.py:318\u001b[0m, in \u001b[0;36mGVPConv.forward\u001b[0;34m(self, x, edge_index, edge_attr)\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[39m\u001b[39m\u001b[39m'''\u001b[39;00m\n\u001b[1;32m    313\u001b[0m \u001b[39m:param x: tuple (s, V) of `torch.Tensor`\u001b[39;00m\n\u001b[1;32m    314\u001b[0m \u001b[39m:param edge_index: array of shape [2, n_edges]\u001b[39;00m\n\u001b[1;32m    315\u001b[0m \u001b[39m:param edge_attr: tuple (s, V) of `torch.Tensor`\u001b[39;00m\n\u001b[1;32m    316\u001b[0m \u001b[39m'''\u001b[39;00m\n\u001b[1;32m    317\u001b[0m x_s, x_v \u001b[39m=\u001b[39m x\n\u001b[0;32m--> 318\u001b[0m message \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpropagate(edge_index, \n\u001b[1;32m    319\u001b[0m             s\u001b[39m=\u001b[39;49mx_s, v\u001b[39m=\u001b[39;49mx_v\u001b[39m.\u001b[39;49mreshape(x_v\u001b[39m.\u001b[39;49mshape[\u001b[39m0\u001b[39;49m], \u001b[39m3\u001b[39;49m\u001b[39m*\u001b[39;49mx_v\u001b[39m.\u001b[39;49mshape[\u001b[39m1\u001b[39;49m]),\n\u001b[1;32m    320\u001b[0m             edge_attr\u001b[39m=\u001b[39;49medge_attr)\n\u001b[1;32m    321\u001b[0m \u001b[39mreturn\u001b[39;00m _split(message, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvo)\n",
      "File \u001b[0;32m~/miniconda3/envs/inverse/lib/python3.9/site-packages/torch_geometric/nn/conv/message_passing.py:463\u001b[0m, in \u001b[0;36mMessagePassing.propagate\u001b[0;34m(self, edge_index, size, **kwargs)\u001b[0m\n\u001b[1;32m    461\u001b[0m     \u001b[39mif\u001b[39;00m res \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    462\u001b[0m         msg_kwargs \u001b[39m=\u001b[39m res[\u001b[39m0\u001b[39m] \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(res, \u001b[39mtuple\u001b[39m) \u001b[39melse\u001b[39;00m res\n\u001b[0;32m--> 463\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmessage(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmsg_kwargs)\n\u001b[1;32m    464\u001b[0m \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_message_forward_hooks\u001b[39m.\u001b[39mvalues():\n\u001b[1;32m    465\u001b[0m     res \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39m, (msg_kwargs, ), out)\n",
      "File \u001b[0;32m~/miniconda3/envs/inverse/lib/python3.9/site-packages/esm/inverse_folding/gvp_modules.py:327\u001b[0m, in \u001b[0;36mGVPConv.message\u001b[0;34m(self, s_i, v_i, s_j, v_j, edge_attr)\u001b[0m\n\u001b[1;32m    325\u001b[0m v_i \u001b[39m=\u001b[39m v_i\u001b[39m.\u001b[39mview(v_i\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], v_i\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\u001b[39m/\u001b[39m\u001b[39m/\u001b[39m\u001b[39m3\u001b[39m, \u001b[39m3\u001b[39m)\n\u001b[1;32m    326\u001b[0m message \u001b[39m=\u001b[39m tuple_cat((s_j, v_j), edge_attr, (s_i, v_i))\n\u001b[0;32m--> 327\u001b[0m message \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmessage_func(message)\n\u001b[1;32m    328\u001b[0m \u001b[39mreturn\u001b[39;00m _merge(\u001b[39m*\u001b[39mmessage)\n",
      "File \u001b[0;32m~/miniconda3/envs/inverse/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/inverse/lib/python3.9/site-packages/torch/nn/modules/container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    138\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 139\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    140\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/inverse/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/inverse/lib/python3.9/site-packages/esm/inverse_folding/gvp_modules.py:159\u001b[0m, in \u001b[0;36mGVP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    157\u001b[0m vh \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwh(v)    \n\u001b[1;32m    158\u001b[0m vn \u001b[39m=\u001b[39m _norm_no_nan(vh, axis\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m, eps\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39meps)\n\u001b[0;32m--> 159\u001b[0m s \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mws(torch\u001b[39m.\u001b[39;49mcat([s, vn], \u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m))\n\u001b[1;32m    160\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscalar_act:\n\u001b[1;32m    161\u001b[0m     s \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscalar_act(s)\n",
      "File \u001b[0;32m~/miniconda3/envs/inverse/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/inverse/lib/python3.9/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# get representation structure for all proteins\n",
    "for cv in tqdm(labels.keys()):\n",
    "    for protein in tqdm(labels[cv]):\n",
    "\n",
    "        path = f\"proteins/{cv}/{protein['id']}.pdb\"\n",
    "        encoder_path = f\"encoder_proteins/{cv}/{protein['id']}\"\n",
    "        \n",
    "        # if protein does not exists in alphafold-db, skip over\n",
    "        if not os.path.exists(path) or os.path.exists(encoder_path + \".npy\"):\n",
    "            continue\n",
    "        \n",
    "        data = {}\n",
    "        \n",
    "        # receive 3-D structure (Atom array (of amino acids) including 3-D coordinates and other info)        \n",
    "        structure = esm.inverse_folding.util.load_structure(path)\n",
    "        \n",
    "        # get coordinates for each amino acid's N-terminal, alpha-carbon and C-terminal (first three in pdb)\n",
    "        coords, native_seq = esm.inverse_folding.util.extract_coords_from_structure(structure)\n",
    "\n",
    "        # get encoder output as structure representation shape: (amino acid, encoder dimension)\n",
    "        rep = esm.inverse_folding.util.get_encoder_output(model, alphabet, coords)\n",
    "        \n",
    "        data[\"data\"] = rep.detach().numpy()\n",
    "        data[\"labels\"] = protein[\"labels\"]\n",
    "        \n",
    "        # create directory and save data as .npy file\n",
    "        os.makedirs(os.path.dirname(encoder_path), exist_ok=True)\n",
    "        np.save(encoder_path, data)\n",
    "        #print(\"saved data for: \", protein['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data': array([[ 0.01676764,  0.10006791, -0.22470556, ...,  0.45647484,\n",
      "         0.14616954, -0.46214253],\n",
      "       [-1.4381772 , -0.7790984 , -0.802924  , ...,  0.30524823,\n",
      "         0.01956351,  0.10650016],\n",
      "       [-1.3221653 , -0.62276614, -0.8991547 , ...,  0.25747558,\n",
      "        -0.02869022,  0.02802757],\n",
      "       ...,\n",
      "       [ 0.16744782,  0.8542608 , -0.05963039, ..., -0.27786946,\n",
      "        -0.11583645, -0.28408888],\n",
      "       [ 0.6397855 ,  0.04167555,  0.37690443, ...,  0.21222025,\n",
      "        -0.20938833, -0.33905762],\n",
      "       [ 1.0578748 , -0.11029857,  0.7357471 , ...,  0.21586058,\n",
      "         0.34215224,  0.15475065]], dtype=float32), 'labels': 'SSSSSSSSSSSSSSSSSSSSSSSSSPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPBBBBBBBOOOOOOOOOOOOOOOOOOOOOOOOOOOOBBBBBBBPPPPPPPPBBBBBBBOOOOOOOOOOOOOOOOOOOOOOOOOBBBBBBBBPPPPPPPPBBBBBBBBOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOBBBBBBBBBPPPPPPBBBBBBBBOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOBBBBBBBBBBPPPPBBBBBBBBBOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOBBBBBBBBBBPPPBBBBBBBBOOOOOOOOOOOOOOOOOOOOOBBBBBBBBBPPPPBBBBBBBBOOOOOOOOOOOOOOOOOOOOOOOOBBBBBBBPP'}\n"
     ]
    }
   ],
   "source": [
    "# # example of loaded dataset\n",
    "# read_dictionary = np.load(encoder_path + \".npy\", allow_pickle='TRUE').item()\n",
    "# print(read_dictionary) # displays \"world\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inverse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
